{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Texture_Networks.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPIgBKR+m8IlpFlmuwg3Yhw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gVrpR3IwjWjO","executionInfo":{"status":"ok","timestamp":1632782375459,"user_tz":420,"elapsed":2387,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","import torchvision.models as visionmodels\n","import torchvision.transforms as transforms\n","import torch.utils.data.dataloader as DataLoader\n","import torch.utils.data.dataset as Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable,grad\n","from torchvision import datasets\n","from collections import *\n","import os\n","from torchvision.io import read_image\n","from datetime import datetime\n","import glob\n","from zipfile import ZipFile\n","from PIL import Image"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"eC-xZr7VzLT2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632782407897,"user_tz":420,"elapsed":31735,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}},"outputId":"ec970733-231e-4bfa-9430-665a37d4071c"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","\n","FLAGS = {} \n","FLAGS['datadir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats'\n","#FLAGS['labeldir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/landscapes.csv'\n","FLAGS['labeldir'] = '/content/gdrive/MyDrive/Colab Notebooks/data/cats_small.csv'\n","FLAGS['batch_size'] = 4\n","FLAGS['learning_rate'] = .1\n","FLAGS['num_epochs'] = 2000\n","FLAGS['im_channels'] = 3\n","FLAGS['noise_scale_factor'] = 0.0\n","FLAGS['loss_scale_lambda'] = 1.\n","FLAGS['im_size'] = 256\n","FLAGS['val_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats/cat-2083492_1280 - Copy (2).jpg' \n","FLAGS['style_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/Paintings/man_with_hat.jpg' \n","FLAGS['val_size_mult'] = 2\n","FLAGS['output_path'] = 'outputs/Texture_net_BN'\n","FLAGS['model_path'] = 'models/Texture_net_BN'\n","FLAGS['output_fname'] = 'test_output_'\n","FLAGS['model_fname'] = 'landscape_model'\n","FLAGS['home_dir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer'"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"nCqoLHYLjdR1","executionInfo":{"status":"ok","timestamp":1632786305730,"user_tz":420,"elapsed":235,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["class ConvLayer(nn.Module): \n","  def __init__(self,in_channels, out_channels): \n","    super().__init__() \n","    self.sample = nn.Conv2d(in_channels, out_channels, kernel_size= 1, stride = 1) \n","    self.main = nn.Sequential(\n","                              nn.Conv2d(in_channels, out_channels, kernel_size = 3, \n","                                        stride = 1,padding =1, padding_mode = 'reflect'), \n","                              nn.BatchNorm2d(out_channels, affine = True), \n","                              nn.LeakyReLU(.2,inplace= True), \n","                              nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n","                                        stride = 1,padding =1, padding_mode = 'reflect'), \n","                              nn.BatchNorm2d(out_channels, affine = True), \n","                              nn.LeakyReLU(.2,inplace= True), \n","                              nn.Conv2d(out_channels, out_channels, kernel_size = 1, \n","                                        stride = 1), \n","                              nn.BatchNorm2d(out_channels, affine = True), \n","                              nn.LeakyReLU(.2,inplace= True)  \n","                              )\n","  def forward(self,input): \n","    return self.main(input)\n","\n","class Join(nn.Module): \n","  def __init__(self,in_channel_1, in_channel_2):\n","    super().__init__()\n","    out_channels = in_channel_1 + in_channel_2\n","    self.up_branch = nn.Sequential(\n","                                   nn.UpsamplingNearest2d(scale_factor= 2), \n","                                   nn.BatchNorm2d(in_channel_1)     \n","                                  )\n","    self.down_branch = nn.BatchNorm2d(in_channel_2,affine = True)\n","  def forward(self, in1, in2):\n","    out1 = self.up_branch(in1)\n","    out2 = self.down_branch(in2)\n","    return torch.cat((out1,out2),dim=1) \n","\n","class ConvJoinBlock(nn.Module): \n","  def __init__(self, in_channels_1, out_channels_1, in_channels_2, out_channels_2): \n","    super().__init__()\n","    self.conv_upper = ConvLayer(in_channels_1,out_channels_1)\n","    self.conv_lower = ConvLayer(in_channels_2,out_channels_2) \n","    self.join = Join(out_channels_1, out_channels_2) \n","\n","  def forward(self,in1, in2): \n","    out1 = self.conv_upper(in1)\n","    out2 = self.conv_lower(in2)\n","    out = self.join(out1,out2)\n","    return out\n","\n","class StyleTransferGenerator(nn.Module): \n","  def __init__(self, num_layers = 6, max_im_size = 512,im_channels = 3, feat_maps = 8): \n","    super().__init__() \n","    self.num_layers = num_layers\n","    self.layers = nn.ModuleList()\n","    branch_feat_maps = 0\n","    for i in range(num_layers-1): \n","      branch_feat_maps += feat_maps\n","      if i == 0: \n","        self.layers.append(ConvJoinBlock(im_channels,branch_feat_maps,im_channels,feat_maps))\n","      else: \n","        self.layers.append(ConvJoinBlock(branch_feat_maps,branch_feat_maps,im_channels, feat_maps)) \n","      \n","    branch_feat_maps += feat_maps\n","      \n","    self.final_block = nn.Sequential(\n","                                    ConvLayer(branch_feat_maps , branch_feat_maps), \n","                                    nn.Conv2d(branch_feat_maps, im_channels,kernel_size = 1),  \n","\n","                                    nn.LeakyReLU(.2,inplace = True)\n","                                    )\n","  \n","\n","  def forward(self,input): \n","    samples = self.get_downsamples(input)\n","    out = samples[-1]  \n","    for i in range(self.num_layers-1,0,-1):\n","      out = self.layers[self.num_layers - (i+1)](out,samples[i-1] )\n","    return self.final_block(out)\n","\n","  def get_downsamples(self,input): \n","    downsample = nn.Upsample(scale_factor=.5, mode = 'nearest') \n","    samples = [input]\n","    for i in range(self.num_layers-1):  \n","      samples.append(downsample(samples[-1]))\n","    return samples\n","\n","class Normalization(nn.Module): \n","  def __init__(self,mean,std): \n","    super().__init__() \n","    self.mean = mean.view(1,-1,1,1)\n","    self.std = std.view(1,-1,1,1) \n","  def forward(self,x):  \n","    return (x-self.mean)/self.std \n","\n","class FeatureNetwork(nn.Module): \n","  def __init__(self,vgg_features): \n","    super().__init__()\n","    self.features = nn.ModuleList(vgg_features).eval() #feature layers of a vgg19_bn network\n","    #self.content_layers = [32,45]\n","    #self.style_layers = [2,9,26,23,30,42]\n","    self.content_layers = [22] \n","    self.style_layers = [1,8,11,20,29]\n","    mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n","    std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n","    self.norm = Normalization(mean,std)\n","  def forward(self,input): \n","    input = self.norm(input)\n","    content = []\n","    styles = [] \n","    for i,module in enumerate(self.features): \n","      input = module(input) \n","      if i in self.content_layers: \n","        content.append(input)\n","      elif i in self.style_layers: \n","        styles.append(input)\n","    return content, styles\n"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETIN3aAdsOQK","executionInfo":{"status":"ok","timestamp":1632786306182,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["\n","\n","def plot_image(input_test,test_image,input_val, val_image, style_image, save_result: bool = True): \n","  fig = plt.figure(figsize = (5,5), dpi = 200) \n","  ax = fig.subplots(3,2)\n","  ax[2,1].imshow(test_image[0].detach().cpu().permute(1,2,0))\n","  ax[2,1].set_title('Stylized Test Image')\n","  ax[1,1].imshow(val_image[0].detach().cpu().permute(1,2,0))\n","  ax[1,1].set_title('Stylized Validation Image')\n","  ax[0,1].imshow(style_image[0].detach().cpu().permute(1,2,0))\n","  ax[0,1].set_title('Style Image')\n","  ax[1,0].imshow(input_val[0].detach().cpu().permute(1,2,0))\n","  ax[1,0].set_title('Validation Image')\n","  ax[2,0].imshow(input_test[0].detach().cpu().permute(1,2,0))\n","  ax[2,0].set_title('Test Image')\n","  if save_result: \n","    os.chdir(FLAGS['home_dir'])\n","    try: \n","      os.chdir(FLAGS['output_path']) \n","    except: \n","      for dir in FLAGS['output_path'].split('/'):\n","        try:\n","          os.chdir(dir)\n","        except: \n","          os.mkdir(dir) \n","          os.chdir(dir)\n","    fname = FLAGS['output_fname'] \n","    num_imgs = len(glob.glob('*.png')) \n","    fname += '_'+str(num_imgs)+'.png'\n","    plt.savefig(fname)\n","    plt.show(block = False)\n","    os.chdir('/content/') \n","  \n","def save_model(model): \n","  os.chdir(FLAGS['home_dir'])\n","  try: \n","    os.chdir(FLAGS['model_path']) \n","  except: \n","    for dir in FLAGS['model_path'].split('/'):\n","      try:\n","        os.chdir(dir)\n","      except: \n","        os.mkdir(dir) \n","        os.chdir(dir)\n","  fname = FLAGS['model_fname']\n","  num_models = len(glob.glob('*.model')) \n","  fname += '_'+str(num_models)+'.model'\n","  torch.save(model.state_dict(), fname)\n","  os.chdir('/content/')"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5JFvpwzuoPo","executionInfo":{"status":"ok","timestamp":1632786306322,"user_tz":420,"elapsed":142,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["zip_loc = '/content/gdrive/MyDrive/Colab Notebooks/data/cats_small.zip'\n","with ZipFile(zip_loc, 'r') as zf: \n","  zf.extractall('data/cats')\n","\n","\n","  "],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"oyshHbaSu2dw","executionInfo":{"status":"ok","timestamp":1632786306322,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["class customDataset(Dataset.Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None,target_transform = None):\n","        '''\n","            we just want an unlabelled image dataset\n","        '''\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return (len(self.img_labels))\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n","        image = read_image(img_path)/255\n","        if self.transform:\n","          image = self.transform(image)\n","        \n","        return image\n","\n","def _initialize_dataset(data_path = None, label_path = None, data_transform = None,target_transform = None):\n","    dataset = customDataset(label_path, data_path, transform=data_transform,target_transform = target_transform)\n","    training_set = torch.utils.data.DataLoader(\n","         dataset, batch_size=FLAGS['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n","    return training_set,dataset\n","\n"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"zM6ilzHNxSwU","executionInfo":{"status":"ok","timestamp":1632786306436,"user_tz":420,"elapsed":115,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["\n","def contentLoss(contents, target_content): \n","  content_loss = 0 \n","  for content,target in zip(contents, target_content): \n","    content_loss+= F.mse_loss(content,target) \n","  return content_loss\n","\n","###computes gram matrix\n","def gram_matrix(A): \n","  N,C, h, w = A.shape\n","  if N == 1:\n","    A = A.view(C,-1)\n","    G = torch.mm(A,A.t())\n","  else: \n","    A = A.view(N,C,-1)\n","    G = torch.bmm(A,A.transpose(1,2))\n","  return G.div(C*h*w) #returns CxC normalized gram matrix\n","#computs gram matrix loss based on euclidean distance\n","def gramMSELoss(input,target): \n","  G = gram_matrix(input)\n","  return F.mse_loss(G,target) \n","#compute Frobenius based loss\n","def gramFrobLoss(input,target): \n","  G = gram_matrix(input) \n","  return torch.linalg.norm(G-target,'fro',(1,2)).sum()\n","###computes styleLosses between set of style representations of the target and source\n","###can specify the mode between frobenius and 2-norm based\n","def styleLoss(styles,target_styles,mode :str = 'fro'): \n","  style_loss = 0 \n","  for style,target in zip(styles,target_styles): \n","    if mode == 'fro': \n","      style_loss += gramFrobLoss(style,target) \n","    elif mode == 'mse': \n","      style_loss += gramMSELoss(style,target) \n","  return style_loss\n","  \n","def train(feature_net,       #network for extracting features\n","                  generator, \n","                  optimizer, \n","                  scheduler, \n","                  train_loader,\n","                  val_image,                    #content target image\n","                  style_image\n","                  ): \n","  for p in feature_net.parameters(): \n","    p.requires_grad = False\n","  style_input = Variable(style_image,requires_grad = False).to(device)\n","  val_input = Variable(val_image, requires_grad = False).to(device)\n","  _,target_styles = feature_net(style_input)\n","  fixed_noise = torch.rand(val_input.shape[0],FLAGS['im_channels'], FLAGS['im_size']*FLAGS['val_size_mult'], FLAGS['im_size']*FLAGS['val_size_mult'],device = device)*FLAGS['noise_scale_factor']\n","  val_input += fixed_noise\n","  val_input.clip_(0,1)\n","  for i,A in enumerate(target_styles): \n","    target_styles[i] = gram_matrix(A.detach()).tile(FLAGS['batch_size'],1,1)\n","  #for i,A in enumerate(target_content):\n","  #  target_content[i] = A.detach().tile(training_batch_size,1,1,1)\n","    \n","  for epoch in range(FLAGS['num_epochs']): \n","    for p in generator.parameters(): \n","      p.requires_grad = True\n","    for i,image in enumerate(train_loader): \n","      batch_size = len(image)\n","      if batch_size != FLAGS['batch_size']: \n","        break\n","      generator.train()\n","      input = Variable(image).to(device)  \n","      nz = Variable(torch.rand(FLAGS['batch_size'],FLAGS['im_channels'],FLAGS['im_size'],FLAGS['im_size'],device = device),requires_grad = False)*.1\n","    \n","      input.clip_(0,1)\n","      optimizer.zero_grad() \n","      '''\n","          forward pass\n","      '''\n","      gen_output = generator(input)\n","      '''\n","        generate test style and content \n","      '''\n","      contents,styles = feature_net(gen_output) \n","      '''\n","        generate actual content\n","      '''\n","      target_content, _  = feature_net(input)\n","      style_loss = styleLoss(styles, target_styles, mode = 'mse')*1e8\n","      content_loss = contentLoss(contents, target_content)*1e3\n","    \n","      \n","      loss = style_loss+ content_loss\n","      loss.backward()\n","      optimizer.step() \n","    scheduler.step()\n","    if (epoch+1) % 10 == 0: \n","      print('stats:{}, {},{}'.format(loss.detach(),style_loss.detach(), content_loss.detach()))\n","    if (epoch+1) % 10 == 0: \n","      generator.eval()\n","      for p in generator.parameters(): \n","        p.requires_grad = False\n","      test_out = generator(val_input)\n","      plot_image(input,gen_output,val_input, test_out, style_input)\n","    if (epoch+1)%10 == 0: \n","      save_model(generator)\n","\n"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"VadHb74G1uQ7","executionInfo":{"status":"ok","timestamp":1632786306436,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["def init_weights(m):\n","    if isinstance(m,nn.Conv2d):\n","        torch.nn.init.xavier_uniform_(m.weight,gain = np.sqrt(2))\n","        if m.bias.data is not None: \n","          m.bias.data.fill_(1.)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"17Rg44g2zeFtbc74wqWqO4vfOUtPEq3QV"},"id":"1Bt6CXdOsS0p","outputId":"420939a7-75c7-452f-e0f6-53fa1e08c1c4"},"source":["if __name__ == '__main__': \n","  os.chdir('/content/')\n","  %ls \n","  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","  vgg_net = visionmodels.vgg19(pretrained=True).to(device)\n","  vgg_net.eval()\n","  feature_net = FeatureNetwork(list(vgg_net.features)).to(device) \n","  noise_scale_factor = .01\n","  generator = StyleTransferGenerator(num_layers = 6,im_channels = FLAGS['im_channels'], feat_maps = 8).to(device)\n","  generator.apply(init_weights)\n","  optimizer = optim.Adam(generator.parameters(), lr = .1, betas = (.5,.999))\n","  def lmbda(epoch): \n","    if epoch == 1000: \n","      return .1 \n","    if epoch > 1000: \n","      if epoch %200 == 0: \n","        return .1\n","      else: \n","        return 1 \n","    else: \n","      return 1\n","  scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda = lmbda)\n","\n","  im_size = FLAGS['im_size']\n","  test_mult = FLAGS['val_size_mult']\n","\n","  transform = transforms.Compose([ transforms.Resize(im_size), transforms.CenterCrop((im_size,im_size))])\n","  val_transform = transforms.Compose([transforms.Resize(im_size*test_mult),transforms.CenterCrop((test_mult*im_size,test_mult*im_size))])\n","\n","  train_loader,dataset = _initialize_dataset(data_path = FLAGS['datadir'], label_path = FLAGS['labeldir'],\\\n","                                data_transform = transform)\n","  \n","  val_image = val_transform(read_image(FLAGS['val_image_path'])/255).unsqueeze(0)\n","  style_image =transforms.ToTensor()(transform(Image.open(FLAGS['style_image_path']).convert('RGB'))).unsqueeze(0)\n","\n","  output = train(feature_net,       #network for extracting features\n","                  generator, \n","                  optimizer, \n","                  scheduler, \n","                  train_loader,\n","                  val_image,                    #content target image\n","                  style_image\n","                  )\n","      \n","  # fig = plt.figure(figsize = (5,5), dpi = 200) \n","  # ax = fig.subplots(2,2)\n","  # ax[1,1].imshow(output[0].detach().cpu().permute(1,2,0)) \n","  # ax[1,0].imshow(content_image[0].detach().cpu().permute(1,2,0))\n","  # ax[0,1].imshow(style_image[0].detach().cpu().permute(1,2,0))\n","  # plt.show(block = False)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"ovVUbjHS9cin"},"source":[""],"execution_count":null,"outputs":[]}]}