{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Instance_norm_texture_nets","provenance":[],"authorship_tag":"ABX9TyN8iv9Av0AnS2c6smRZ9lZg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"46zCZDFfUY_b","executionInfo":{"status":"ok","timestamp":1632786496918,"user_tz":420,"elapsed":3549,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","import torchvision.models as visionmodels\n","import torchvision.transforms as transforms\n","import torch.utils.data.dataloader as DataLoader\n","import torch.utils.data.dataset as Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable,grad\n","from torchvision import datasets\n","from collections import *\n","import os\n","from torchvision.io import read_image\n","from datetime import datetime\n","import glob\n","from zipfile import ZipFile\n","from PIL import Image"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"hAWNV56iUhUy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632786512939,"user_tz":420,"elapsed":16022,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}},"outputId":"c3eca621-b2e1-4483-9226-269b91cf7da9"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","\n","FLAGS = {} \n","FLAGS['datadir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats'\n","#FLAGS['labeldir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/landscapes.csv'\n","FLAGS['labeldir'] = '/content/gdrive/MyDrive/Colab Notebooks/data/cats_small.csv'\n","FLAGS['batch_size'] = 4\n","FLAGS['learning_rate'] = .1\n","FLAGS['num_epochs'] = 2000\n","FLAGS['im_channels'] = 3\n","FLAGS['noise_scale_factor'] = .01\n","FLAGS['loss_scale_lambda'] = 1e6\n","FLAGS['im_size'] = 256\n","FLAGS['val_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats/cat-2083492_1280 - Copy (2).jpg' \n","FLAGS['style_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/Paintings/man_with_hat.jpg' \n","FLAGS['val_size_mult'] = 2\n","FLAGS['output_path'] = 'outputs/Texture_net_IN'\n","FLAGS['model_path'] = 'models/Texture_net_IN'\n","FLAGS['output_fname'] = 'test_output_swirt'\n","FLAGS['model_fname'] = 'painting_model'\n","FLAGS['home_dir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer'"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"ZELmlWr_Ui3p","executionInfo":{"status":"ok","timestamp":1632787915159,"user_tz":420,"elapsed":336,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["class ConvLayer(nn.Module): \n","  def __init__(self,in_channels, out_channels): \n","    super().__init__() \n","    self.main = nn.Sequential(\n","                              nn.Conv2d(in_channels, out_channels, kernel_size = 3, \n","                                        stride = 1,padding =1, padding_mode = 'reflect'), \n","                              nn.InstanceNorm2d(out_channels,affine = True), \n","                              nn.LeakyReLU(.2,inplace= True), \n","                              nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n","                                        stride = 1,padding =1, padding_mode = 'reflect'), \n","                              nn.InstanceNorm2d(out_channels,affine = True), \n","                              nn.LeakyReLU(.2,inplace= True), \n","                              nn.Conv2d(out_channels, out_channels, kernel_size = 1, \n","                                        stride = 1, bias = True), \n","                              nn.InstanceNorm2d(out_channels,affine = True), \n","                              nn.LeakyReLU(.2,inplace= True)     \n","                              )\n","  def forward(self,input): \n","    return self.main(input) \n","\n","class Join(nn.Module): \n","  def __init__(self,in_channel_1, in_channel_2):\n","    super().__init__()\n","    out_channels = in_channel_1 + in_channel_2\n","    self.up_branch = nn.Sequential(\n","                                   nn.UpsamplingNearest2d(scale_factor= 2), \n","                                   nn.InstanceNorm2d(in_channel_1,affine = True)     \n","                                  )\n","    self.down_branch = nn.InstanceNorm2d(in_channel_2,affine = True)\n","  def forward(self, in1, in2):\n","    out1 = self.up_branch(in1)\n","    out2 = self.down_branch(in2)\n","    return torch.cat((out1,out2),dim=1) \n","\n","class ConvJoinBlock(nn.Module): \n","  def __init__(self, in_channels_1, out_channels_1, in_channels_2, out_channels_2): \n","    super().__init__()\n","    self.conv_upper = ConvLayer(in_channels_1,out_channels_1)\n","    self.conv_lower = ConvLayer(in_channels_2,out_channels_2) \n","    self.join = Join(out_channels_1, out_channels_2) \n","\n","  def forward(self,in1, in2): \n","    out1 = self.conv_upper(in1)\n","    out2 = self.conv_lower(in2)\n","    out = self.join(out1,out2)\n","    return out\n","\n","class StyleTransferGenerator(nn.Module): \n","  def __init__(self, num_layers = 6, max_im_size = 512,im_channels = 3, feat_maps = 8): \n","    super().__init__() \n","    self.num_layers = num_layers\n","    self.layers = nn.ModuleList()\n","    branch_feat_maps = 0\n","    for i in range(num_layers-1): \n","      branch_feat_maps += feat_maps\n","      if i == 0: \n","        self.layers.append(ConvJoinBlock(im_channels,branch_feat_maps,im_channels,feat_maps))\n","      else: \n","        self.layers.append(ConvJoinBlock(branch_feat_maps,branch_feat_maps,im_channels, feat_maps)) \n","      \n","    branch_feat_maps += feat_maps\n","      \n","    self.final_block = nn.Sequential(\n","                                    ConvLayer(branch_feat_maps , branch_feat_maps), \n","                                    nn.Conv2d(branch_feat_maps, im_channels,kernel_size = 1), \n","                                    nn.LeakyReLU(.2,inplace = False)\n","                                    )\n","  \n","\n","  def forward(self,input): \n","    samples = self.get_downsamples(input)\n","    out = samples[-1]\n","    for i in range(self.num_layers-1,0,-1):\n","      out = self.layers[self.num_layers - (i+1)](out,samples[i-1])\n","    return self.final_block(out)\n","\n","  def get_downsamples(self,input): \n","    downsample = nn.Upsample(scale_factor=.5, mode = 'nearest') \n","    samples = [input]\n","    for i in range(self.num_layers-1):  \n","      samples.append(downsample(samples[-1]))\n","    return samples\n","\n","class Normalization(nn.Module): \n","  def __init__(self,mean,std): \n","    super().__init__() \n","    self.mean = mean.view(1,-1,1,1)\n","    self.std = std.view(1,-1,1,1) \n","  def forward(self,x):  \n","    return (x-self.mean)/self.std \n","\n","class FeatureNetwork(nn.Module): \n","  def __init__(self,vgg_features): \n","    super().__init__()\n","    self.features = vgg_features #feature layers of a vgg19_bn network\n","    #self.content_layers = [32]\n","    #self.style_layers = [2,9,26,23,30,42]\n","    self.content_layers = [22] \n","    self.style_layers = [1,6,11,20,29]\n","    mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n","    std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n","    self.norm = Normalization(mean,std)\n","  def forward(self,input): \n","    input = self.norm(input)\n","    content = []\n","    styles = [] \n","    for i,module in enumerate(self.features): \n","      input = module(input) \n","      if i in self.content_layers: \n","        content.append(input)\n","      elif i in self.style_layers: \n","        styles.append(input)\n","    return content, styles\n"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"muf_kqVxUtwr","executionInfo":{"status":"ok","timestamp":1632787915583,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["def gram_matrix(A): \n","  N,C, h, w = A.shape\n","  if N == 1:\n","    A = A.view(C,-1)\n","    G = torch.mm(A,A.t())\n","  else: \n","    A = A.view(N,C,-1)\n","    G = torch.bmm(A,A.transpose(1,2))\n","  return G.div(C*h*w) #returns CxC normalized gram matrix\n","\n","def gramMSELoss(input,target): \n","  #assuming target is already a gram matrix \n","  G = gram_matrix(input)\n","  return F.mse_loss(G,target) \n","\n","def plot_image(input_test,test_image,input_val, val_image, style_image, save_result: bool = True): \n","  fig = plt.figure(figsize = (5,5), dpi = 200) \n","  ax = fig.subplots(3,2)\n","  ax[2,1].imshow(test_image[0].detach().cpu().permute(1,2,0))\n","  ax[2,1].set_title('Stylized Test Image')\n","  ax[1,1].imshow(val_image[0].detach().cpu().permute(1,2,0))\n","  ax[1,1].set_title('Stylized Validation Image')\n","  ax[0,1].imshow(style_image[0].detach().cpu().permute(1,2,0))\n","  ax[0,1].set_title('Style Image')\n","  ax[1,0].imshow(input_val[0].detach().cpu().permute(1,2,0))\n","  ax[1,0].set_title('Validation Image')\n","  ax[2,0].imshow(input_test[0].detach().cpu().permute(1,2,0))\n","  ax[2,0].set_title('Test Image')\n","  if save_result: \n","    os.chdir(FLAGS['home_dir'])\n","    try: \n","      os.chdir(FLAGS['output_path']) \n","    except: \n","      for dir in FLAGS['output_path'].split('/'):\n","        try:\n","          os.chdir(dir)\n","        except: \n","          os.mkdir(dir) \n","          os.chdir(dir)\n","    fname = FLAGS['output_fname'] \n","    num_imgs = len(glob.glob('*.png')) \n","    fname += '_'+str(num_imgs)+'.png'\n","    plt.savefig(fname)\n","    plt.show(block = False)\n","    os.chdir('/content/') \n","  \n","def save_model(model): \n","  os.chdir(FLAGS['home_dir'])\n","  try: \n","    os.chdir(FLAGS['model_pathv']) \n","  except: \n","    for dir in FLAGS['model_path'].split('/'):\n","      try:\n","        os.chdir(dir)\n","      except: \n","        os.mkdir(dir) \n","        os.chdir(dir)\n","  fname = FLAGS['model_fname']\n","  num_models = len(glob.glob('*.model')) \n","  fname += '_'+str(num_models)+'.model'\n","  torch.save(model.state_dict(), fname)\n","  os.chdir('/content/')"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"0XlY9zmCUuHA","executionInfo":{"status":"ok","timestamp":1632787916025,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["zip_loc = '/content/gdrive/MyDrive/Colab Notebooks/data/cats_small.zip'\n","with ZipFile(zip_loc, 'r') as zf: \n","  zf.extractall('data/cats')\n","\n","\n","  "],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"l47PoDFbUvWF","executionInfo":{"status":"ok","timestamp":1632787916187,"user_tz":420,"elapsed":4,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["class customDataset(Dataset.Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None,target_transform = None):\n","        '''\n","            we just want an unlabelled image dataset\n","        '''\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return (len(self.img_labels))\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n","        image = read_image(img_path)/255\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        return image\n","\n","def _initialize_dataset(data_path = None, label_path = None, data_transform = None,target_transform = None):\n","    dataset = customDataset(label_path, data_path, transform=data_transform,target_transform = target_transform)\n","    training_set = torch.utils.data.DataLoader(\n","         dataset, batch_size=FLAGS['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n","    return training_set,dataset\n"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"lvYlzvNEVE6H","executionInfo":{"status":"ok","timestamp":1632787916187,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["def init_weights(m):\n","    if isinstance(m,nn.Conv2d):\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        if m.bias is not None: \n","          m.bias.data.fill_(0.)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"rAo49af7UwkY","executionInfo":{"status":"ok","timestamp":1632787916188,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["def train(feature_net,       #network for extracting features\n","                  generator, \n","                  optimizer, \n","                  scheduler, \n","                  train_loader,\n","                  val_image,                    #content target image\n","                  style_image                      #style target image\n","                  ): \n","  for p in feature_net.parameters(): \n","    p.requires_grad = False\n","  style_input = Variable(style_image,requires_grad = False).to(device)\n","  val_input = Variable(val_image, requires_grad = False).to(device)\n","  _,target_styles = feature_net(style_input)\n"," \n","  for i,A in enumerate(target_styles): \n","    target_styles[i] = gram_matrix(A.detach()).tile(FLAGS['batch_size'],1,1)\n","  #for i,A in enumerate(target_content):\n","  #  target_content[i] = A.detach().tile(training_batch_size,1,1,1)\n","    \n","  for epoch in range(FLAGS['num_epochs']): \n","    for p in generator.parameters(): \n","      p.requires_grad = True\n","    for i,image in enumerate(train_loader): \n","      batch_size = len(image)\n","      if batch_size != FLAGS['batch_size']: \n","        break\n","\n","      input = Variable(image).to(device)  \n","      nz = Variable(torch.rand(FLAGS['batch_size'],FLAGS['im_channels'],FLAGS['im_size'],FLAGS['im_size'],device = device),requires_grad = False)*FLAGS['noise_scale_factor']\n","      input += nz \n","      input.clip_(0,1)\n","      optimizer.zero_grad() \n","      '''\n","          forward pass\n","      '''\n","      gen_output = generator(input)\n","      \n","      gen_output.clip_(0,1)\n","      '''\n","        generate test style and content \n","      '''\n","      contents,styles = feature_net(gen_output) \n","      '''\n","        generate actual content\n","      '''\n","      target_content, _  = feature_net(input)\n","      style_loss = 0\n","      content_loss = 0\n","      for style,target in zip(styles,target_styles): \n","        style_loss += gramMSELoss(style,target)*1e9\n","      for content, target in zip(contents, target_content): \n","        content_loss += F.mse_loss(content,target)*1e3\n","      \n","      loss = style_loss + content_loss\n","      loss.backward()\n","      optimizer.step() \n","    scheduler.step()\n","    if (epoch+1) % 10 == 0: \n","      print('stats:{}'.format(loss.detach()))\n","    if (epoch+1) % 10 == 0: \n","      for p in generator.parameters(): \n","        p.requires_grad = False\n","      test_out = generator(val_input)\n","      plot_image(input,gen_output,val_input, test_out, style_input)\n","\n","    if (epoch+1)%10 == 0: \n","      save_model(generator)\n","\n"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1G116DCfbhEjOEEvk5JEa4NJb7rhs0bOQ"},"id":"ftKZXy64U6K-","executionInfo":{"status":"error","timestamp":1632788086991,"user_tz":420,"elapsed":170806,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}},"outputId":"0436ebcc-5ac3-4ac4-c830-081591b30e3e"},"source":["if __name__ == '__main__': \n","  os.chdir('/content/')\n","  %ls \n","  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","  vgg_net = visionmodels.vgg19(pretrained=True).to(device)\n","  vgg_net.eval()\n","  feature_net = FeatureNetwork(list(vgg_net.features)).to(device) \n","  generator = StyleTransferGenerator(num_layers = 6,im_channels = FLAGS['im_channels'], feat_maps = 8).to(device)\n","  generator.apply(init_weights)\n","  optimizer = optim.Adam(generator.parameters(), lr = FLAGS['learning_rate'], betas = (.5,.999))\n","  def lmbda(epoch): \n","    if epoch == 1000: \n","      return .1 \n","    if epoch > 1000: \n","      if epoch %200 == 0: \n","        return .1\n","      else: \n","        return 1 \n","    else: \n","      return 1\n","  scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda = lmbda)\n","\n","  im_size = FLAGS['im_size']\n","  test_mult = FLAGS['val_size_mult']\n","\n","  transform = transforms.Compose([ transforms.Resize(im_size), transforms.CenterCrop((im_size,im_size))])\n","  val_transform = transforms.Compose([transforms.Resize(im_size*test_mult),transforms.CenterCrop((test_mult*im_size,test_mult*im_size))])\n","\n","  train_loader,dataset = _initialize_dataset(data_path = FLAGS['datadir'], label_path = FLAGS['labeldir'],\\\n","                                data_transform = transform)\n","  \n","  val_image = val_transform(read_image(FLAGS['val_image_path'])/255).unsqueeze(0)\n","  style_image =transforms.ToTensor()(transform(Image.open(FLAGS['style_image_path']).convert('RGB'))).unsqueeze(0)\n","\n","  output = train(feature_net,       #network for extracting features\n","                  generator, \n","                  optimizer, \n","                  scheduler, \n","                  train_loader,\n","                  val_image,                    #content target image\n","                  style_image                      #style target image\n","                  )\n","      "],"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"D9mqDQ8W95Tx","executionInfo":{"status":"aborted","timestamp":1632788086990,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":[""],"execution_count":null,"outputs":[]}]}