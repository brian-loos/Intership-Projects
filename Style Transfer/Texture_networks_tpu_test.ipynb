{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Texture_networks_tpu_test.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMhtvAPPK3uX1FOGFp9gD7T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"mWeBWRRBy06e","executionInfo":{"status":"ok","timestamp":1632364611234,"user_tz":420,"elapsed":302,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["import os\n","assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZFSBofo2zgi-","executionInfo":{"status":"ok","timestamp":1632364643900,"user_tz":420,"elapsed":32428,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}},"outputId":"e2c4b46f-734c-4372-8c1e-bdf8982288aa"},"source":["!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch-xla==1.9\n","  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n","\u001b[K     |████████████████████████████████| 149.9 MB 26 kB/s \n","\u001b[?25hRequirement already satisfied: cloud-tpu-client==0.10 in /usr/local/lib/python3.7/dist-packages (0.10)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n","Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (1.8.0)\n","Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.35.0)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n","Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n","Installing collected packages: torch-xla\n","  Attempting uninstall: torch-xla\n","    Found existing installation: torch-xla 1.9\n","    Uninstalling torch-xla-1.9:\n","      Successfully uninstalled torch-xla-1.9\n","Successfully installed torch-xla-1.9.1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVrpR3IwjWjO","executionInfo":{"status":"ok","timestamp":1632364646532,"user_tz":420,"elapsed":2635,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}},"outputId":"a7b03e7f-f3cc-4120-f86f-3c332ffaed2d"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","import torchvision.models as visionmodels\n","import torchvision.transforms as transforms\n","import torch.utils.data.dataloader as DataLoader\n","import torch.utils.data.dataset as Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable,grad\n","from torchvision import datasets\n","from collections import *\n","import os\n","from torchvision.io import read_image\n","from datetime import datetime\n","import glob\n","from zipfile import ZipFile\n","\n","import torch_xla\n","import torch_xla.core.xla_model as xm\n","import torch_xla.debug.metrics as met\n","import torch_xla.distributed.parallel_loader as pl\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","import torch_xla.utils.utils as xu\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:TPU has started up successfully with version pytorch-1.9.1\n"]}]},{"cell_type":"code","metadata":{"id":"eC-xZr7VzLT2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632364646628,"user_tz":420,"elapsed":100,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}},"outputId":"c56a26cd-c979-47be-bc24-a158ea6d1e3a"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","zip_loc = '/content/gdrive/MyDrive/Colab Notebooks/data/cats_small.zip'\n","with ZipFile(zip_loc, 'r') as zf: \n","  zf.extractall('data/cats')\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"X5VhXU8l0FJb","executionInfo":{"status":"ok","timestamp":1632364646629,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["FLAGS = {}\n","FLAGS['datadir'] = 'data/cats'\n","FLAGS['labeldir'] = '/content/gdrive/MyDrive/Colab Notebooks/data/cats_small.csv'\n","FLAGS['batch_size'] = 4\n","FLAGS['num_workers'] = 4\n","FLAGS['learning_rate'] = .0001\n","FLAGS['num_epochs'] = 200\n","FLAGS['num_cores'] = 8\n","FLAGS['noise_dim'] = 3\n","FLAGS['noise_scale_factor'] = .01\n","FLAGS['loss_scale_lambda'] = 2e6\n","FLAGS['im_size'] = 512\n","FLAGS['val_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats/val_cat.jpg' \n","FLAGS['style_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/monet/07.jpg' \n","FLAGS['val_size_mult'] = 2"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"lIjYiySH0R-M","executionInfo":{"status":"ok","timestamp":1632364646904,"user_tz":420,"elapsed":277,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["from matplotlib.pyplot import imshow\n","from matplotlib import pyplot as plt\n","from IPython import display \n","\n","from google.colab.patches import cv2_imshow\n","import cv2\n","    \n","RESULT_IMG_PATH = '/tmp/test_result.png'\n","\n","def plot_results(image,epoch):\n","    fig = plt.figure(figsize= (5,5), dpi = 200) \n","    ax = fig.subplots()\n","    ax.imshow(image[0].permute(1,2,0))\n","    plt.savefig('/tmp/test_result_{}.png'.format(epoch), transparent=True)\n","    plt.show(block = False)\n","def display_results(epoch):\n","    img = cv2.imread('/tmp/test_result_{}.png'.format(epoch), cv2.IMREAD_UNCHANGED)\n","    cv2_imshow(img)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCqoLHYLjdR1","executionInfo":{"status":"ok","timestamp":1632364647523,"user_tz":420,"elapsed":620,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["class ConvLayer(nn.Module): \n","  def __init__(self,in_channels, out_channels): \n","    super().__init__() \n","    self.main = nn.Sequential(\n","                              nn.Conv2d(in_channels, out_channels, kernel_size = 3, \n","                                        stride = 1,padding =1, padding_mode = 'reflect', bias = True), \n","                              nn.BatchNorm2d(out_channels), \n","                              nn.LeakyReLU(.2,inplace= False), \n","                              nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n","                                        stride = 1,padding =1, padding_mode = 'reflect', bias = True), \n","                              nn.BatchNorm2d(out_channels), \n","                              nn.LeakyReLU(.2,inplace= False), \n","                              nn.Conv2d(out_channels, out_channels, kernel_size = 1, \n","                                        stride = 1, bias = True), \n","                              nn.BatchNorm2d(out_channels), \n","                              nn.LeakyReLU(.2,inplace= False)     \n","                              )\n","  def forward(self,input): \n","    return self.main(input) \n","\n","class Join(nn.Module): \n","  def __init__(self,in_channel_1, in_channel_2):\n","    super().__init__()\n","    out_channels = in_channel_1 + in_channel_2\n","    self.up_branch = nn.Sequential(\n","                                   nn.UpsamplingNearest2d(scale_factor= 2), \n","                                   nn.BatchNorm2d(in_channel_1)     \n","                                  )\n","    self.down_branch = nn.BatchNorm2d(in_channel_2)\n","  def forward(self, in1, in2):\n","    out1 = self.up_branch(in1)\n","    out2 = self.down_branch(in2)\n","    return torch.cat((out1,out2),dim=1) \n","\n","class ConvJoinBlock(nn.Module): \n","  def __init__(self, in_channels_1, out_channels_1, in_channels_2, out_channels_2): \n","    super().__init__()\n","    self.conv_upper = ConvLayer(in_channels_1,out_channels_1)\n","    self.conv_lower = ConvLayer(in_channels_2,out_channels_2) \n","    self.join = Join(out_channels_1, out_channels_2) \n","\n","  def forward(self,in1, in2): \n","    out1 = self.conv_upper(in1)\n","    out2 = self.conv_lower(in2)\n","    out = self.join(out1,out2)\n","    return out\n","\n","class StyleTransferGenerator(nn.Module): \n","  def __init__(self, num_layers = 6, max_im_size = 512,im_channels = 3,noise_channels = 3, feat_maps = 8): \n","    super().__init__() \n","    self.num_layers = num_layers\n","    self.layers = nn.ModuleList()\n","    branch_feat_maps = 0\n","    for i in range(num_layers-1): \n","      branch_feat_maps += feat_maps\n","      if i == 0: \n","        self.layers.append(ConvJoinBlock(im_channels,branch_feat_maps,im_channels,feat_maps))\n","      else: \n","        self.layers.append(ConvJoinBlock(branch_feat_maps,branch_feat_maps,im_channels, feat_maps)) \n","      \n","    branch_feat_maps += feat_maps\n","      \n","    self.final_block = nn.Sequential(\n","                                    ConvLayer(branch_feat_maps , branch_feat_maps), \n","                                    nn.Conv2d(branch_feat_maps, im_channels,kernel_size = 1, bias = True), \n","                                    nn.BatchNorm2d(im_channels), \n","                                    nn.LeakyReLU(.2,inplace = False)\n","                                    )\n","  \n","\n","  def forward(self,input): \n","    samples = self.get_downsamples(input)\n","    out = samples[-1]     \n","    for i in range(self.num_layers-1,0,-1):\n","      out = self.layers[self.num_layers - (i+1)](out, samples[i-1])\n","    return self.final_block(out)\n","\n","  def get_downsamples(self,input): \n","    downsample = nn.Upsample(scale_factor=.5, mode = 'bilinear') \n","    samples = [input]\n","    for i in range(self.num_layers-1):  \n","      samples.append(downsample(samples[-1]))\n","    return samples\n","\n","class Normalization(nn.Module): \n","  def __init__(self,mean,std): \n","    super().__init__() \n","    self.mean = mean.view(1,-1,1,1)\n","    self.std = std.view(1,-1,1,1) \n","  def forward(self,x):  \n","    return (x-self.mean)/self.std \n","\n","class FeatureNetwork(nn.Module): \n","  def __init__(self,vgg_features): \n","    super().__init__()\n","    self.features = vgg_features #feature layers of a vgg19_bn network\n","    self.content_layers = [32]\n","    self.style_layers = [2,9,26,23,30,42]\n","    self.mean = torch.tensor([0.485, 0.456, 0.406])\n","    self.std = torch.Tensor([0.229, 0.224, 0.225])\n","    self.norm = Normalization(self.mean,self.std)\n","  def forward(self,input): \n","    input = self.norm(input)\n","    content = []\n","    styles = [] \n","    for i,module in enumerate(self.features): \n","      input = module(input) \n","      if i in self.content_layers: \n","        content.append(input)\n","      elif i in self.style_layers: \n","        styles.append(input)\n","    return content, styles\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"oyshHbaSu2dw","executionInfo":{"status":"ok","timestamp":1632364647523,"user_tz":420,"elapsed":4,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["class customDataset(Dataset.Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None,target_transform = None):\n","        '''\n","            we just want an unlabelled image dataset\n","        '''\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return (len(self.img_labels))\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n","        image = read_image(img_path)/255\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        return image\n","\n","def _initialize_dataset(data_path = None, label_path = None, data_transform = None,target_transform = None):\n","    dataset = customDataset(label_path, data_path, transform=data_transform,target_transform = target_transform)\n","    # training_set = torch.utils.data.DataLoader(\n","    #      dataset, batch_size=FLAGS['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n","    return dataset\n","\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2e8WgkVRAGJB"},"source":["# Need to rewrite training loop for XMA package"]},{"cell_type":"code","metadata":{"id":"K1d-SPusAVFV","executionInfo":{"status":"ok","timestamp":1632364647523,"user_tz":420,"elapsed":3,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["def init_data():\n","  im_size = FLAGS['im_size']\n","  source_data_dir = FLAGS['datadir']\n","  annotation_file = FLAGS['labeldir']\n","  compose = transforms.Compose([ transforms.Resize(im_size), transforms.CenterCrop((im_size,im_size))])\n","  dataset = _initialize_dataset(data_path = source_data_dir, label_path = annotation_file,\\\n","                              data_transform = compose)\n","\n","  return dataset"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"U0BWzR7uKiv6","executionInfo":{"status":"ok","timestamp":1632364647523,"user_tz":420,"elapsed":2,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["def init_weights(m):\n","    if isinstance(m,nn.Conv2d):\n","        torch.nn.init.xavier_normal_(m.weight)\n","        if m.bias.data is not None: \n","          m.bias.data.fill_(0.)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAWxsYZcAF3M","executionInfo":{"status":"ok","timestamp":1632364654027,"user_tz":420,"elapsed":6506,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":["SERIAL_EXEC = xmp.MpSerialExecutor()\n","# Only instantiate model weights once in memory.\n","vgg_net = visionmodels.vgg19_bn(pretrained=True)\n","#feature_net = FeatureNetwork(list(vgg_net.features))\n","generator = StyleTransferGenerator(num_layers = 6,im_channels = 3,noise_channels = FLAGS['noise_dim'], feat_maps = 8)\n","generator.apply(init_weights)\n","\n","WRAPPED_GENERATOR = xmp.MpModelWrapper(generator) \n","#WRAPPED_F_NET = xmp.MpModelWrapper(feature_net)\n","WRAPPED_VGG = xmp.MpModelWrapper(vgg_net)\n","\n","def train(rank): \n","  torch.manual_seed(1) \n","  data = SERIAL_EXEC.run(lambda: init_data()) \n","  train_sampler = torch.utils.data.distributed.DistributedSampler(\n","      data, \n","      num_replicas = xm.xrt_world_size(), \n","      rank = xm.get_ordinal(), \n","      shuffle = True \n","  )\n","\n","  train_loader = torch.utils.data.DataLoader(\n","    data, \n","    batch_size =FLAGS['batch_size'], \n","    sampler = train_sampler, \n","    num_workers = FLAGS['num_workers'], \n","    drop_last = False   \n","  )\n","\n","  num_batches = len(train_loader)\n","  device = xm.xla_device() \n","  G = WRAPPED_GENERATOR.to(device) \n","  #f_net = WRAPPED_F_NET.to(device)\n","  vgg = WRAPPED_VGG.to(device)\n","  feature_net = FeatureNetwork(list(vgg.features)).to(device)\n","  f_net = xmp.MpModelWrapper(feature_net).to(device)\n","  '''\n","    optimizer and disable feature net gradients\n","  '''\n","  optimizer = optim.Adam(G.parameters(), lr = FLAGS['learning_rate'], betas = (0.0,.999)) \n","  for p in f_net.parameters(): \n","    p.requires_grad = False\n","\n","  xm.master_print('mean/std device:'.format(f_net.mean.device, f_net.std.device))\n","\n","  '''\n","    training parameters\n","  '''\n","  noise_dim = FLAGS['noise_dim']\n","  noise_sf = FLAGS['noise_scale_factor']\n","  lmbda = FLAGS['loss_scale_lambda']\n","  num_epochs = FLAGS['num_epochs']\n","  im_size = FLAGS['im_size']\n","  test_mult = FLAGS['val_size_mult']\n","  '''\n","    GRAM MATRIX LOSS \n","  '''\n","  def gram_matrix(A): \n","    N,C, h, w = A.shape\n","    if N == 1:\n","      A = A.view(C,-1)\n","      G = torch.mm(A,A.t())\n","    else: \n","      A = A.view(N,C,-1)\n","      G = torch.bmm(A,A.transpose(1,2))\n","    return G.div(C*h*w) #returns CxC normalized gram matrix\n","\n","  def gramMSELoss(input,target): \n","    #assuming target is already a gram matrix \n","    G = gram_matrix(input)\n","    return F.mse_loss(G,target,reduction = 'mean') \n","\n","  '''\n","    style image and necessary transforms\n","  '''  \n","  #transform = transforms.Compose([ transforms.Resize(im_size), transforms.CenterCrop((im_size,im_size))])\n","  transform = transforms.RandomResizedCrop(im_size, scale = (1,1))\n","  val_transform = transforms.Compose([transforms.Resize(test_mult*im_size),transforms.CenterCrop((test_mult*im_size,test_mult*im_size))])\n","  val_image = val_transform(read_image(FLAGS['val_image_path'])/255).unsqueeze(0)\n","  val_input = Variable(val_image, requires_grad = False).to(device)\n","  fixed_noise = torch.rand(val_input.shape[0],noise_dim, val_input.shape[2], val_input.shape[3],device = device)*noise_sf\n","  \n","\n","  def train_step(optimizer, input,device): \n","    optimizer.zero_grad() \n","\n","    '''\n","      train net\n","    '''\n","    style_image = transform(read_image(FLAGS['style_image_path'])/255).unsqueeze(0)\n","    style_input = Variable(style_image,requires_grad = False).to(device)\n","    _,target_styles = f_net(style_input)\n","    for i,A in enumerate(target_styles): \n","      target_styles[i] = gram_matrix(A.detach()).tile(FLAGS['batch_size'],1,1)\n","\n","    nz = Variable(torch.rand(FLAGS['batch_size'],noise_dim,im_size,im_size,device = device),requires_grad = False)*noise_sf\n","    input += nz\n","    optimizer.zero_grad() \n","    '''\n","        forward pass\n","    '''\n","    gen_output = G(input)\n","    '''\n","      generate test style and content \n","    '''\n","    contents,styles = f_net(gen_output) \n","    '''\n","      generate actual content\n","    '''\n","    target_content, _  = f_net(input)\n","    style_loss = 0\n","    content_loss = 0\n","    for style,target in zip(styles,target_styles): \n","      style_loss += gramMSELoss(style,target)\n","    for content, target in zip(contents, target_content): \n","      content_loss += F.mse_loss(content,target,reduction = 'mean')\n","    \n","    loss = style_loss*lmbda + content_loss\n","    loss.backward()\n","    xm.optimizer_step(optimizer)\n","\n","    return loss.detach()\n","  \n","  def train_loop(loader): \n","    tracker = xm.RateTracker() \n","    G.train() \n","    f_net.eval() \n","\n","    for n_batch, image in enumerate(loader): \n","      if len(image) != FLAGS['batch_size']: \n","        break\n","      input = Variable(image).to(device)  \n","\n","      loss = train_step(optimizer, input, device)\n","\n","    return loss\n","  \n","  for epoch in range(1,num_epochs+1): \n","    loss = train_loop(pl.MpDeviceLoader(train_loader, device))\n","    xm.master_print(\"Epoch {}/{}: Loss: {}\".format(epoch, num_epochs, loss))\n","    if (epoch+1)%10 == 0:\n","      with torch.no_grad(): \n","        G.eval()\n","        xm.do_on_ordinals(plot_results, (G(val_input+fixed_noise).detach(),epoch), (0,)) \n","        #xm.do_on_ordinals(display_results, epoch, (0,))\n","    "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"HOP0no2_GBhJ","executionInfo":{"status":"error","timestamp":1632364742193,"user_tz":420,"elapsed":88174,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}},"outputId":"7c36ceb0-c884-4b55-bcf1-2258d699a8d7"},"source":["def _mp_fn(rank,flags): \n","  global FLAGS\n","  FLAGS = flags\n","  torch.set_default_tensor_type('torch.FloatTensor') \n","  train(rank)\n","\n","xmp.spawn(_mp_fn, args = (FLAGS,), nprocs = FLAGS['num_cores'], start_method= 'fork')"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["mean/std device:\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/200: Loss: 0.03383183106780052\n"]},{"output_type":"stream","name":"stderr","text":["Exception in device=TPU:6: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 329, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    fn(gindex, *args)\n","  File \"<ipython-input-12-673f955cbafa>\", line 5, in _mp_fn\n","    train(rank)\n","  File \"<ipython-input-11-823d6085a09e>\", line 139, in train\n","    loss = train_loop(pl.MpDeviceLoader(train_loader, device))\n","  File \"<ipython-input-11-823d6085a09e>\", line 129, in train_loop\n","    for n_batch, image in enumerate(loader):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 34, in __next__\n","    return self.next()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 46, in next\n","    xm.mark_step()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 718, in mark_step\n","    wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))\n","RuntimeError: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label\n","Exception in device=TPU:3: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 329, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","Exception in device=TPU:7: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    fn(gindex, *args)\n","\n","  File \"<ipython-input-12-673f955cbafa>\", line 5, in _mp_fn\n","    train(rank)\n","  File \"<ipython-input-11-823d6085a09e>\", line 139, in train\n","    loss = train_loop(pl.MpDeviceLoader(train_loader, device))\n","  File \"<ipython-input-11-823d6085a09e>\", line 129, in train_loop\n","    for n_batch, image in enumerate(loader):\n","Traceback (most recent call last):\n","Exception in device=TPU:2: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 34, in __next__\n","    return self.next()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 329, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    fn(gindex, *args)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 46, in next\n","    xm.mark_step()\n","  File \"<ipython-input-12-673f955cbafa>\", line 5, in _mp_fn\n","    train(rank)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 329, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 718, in mark_step\n","    wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))\n","  File \"<ipython-input-11-823d6085a09e>\", line 139, in train\n","    loss = train_loop(pl.MpDeviceLoader(train_loader, device))\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    fn(gindex, *args)\n","RuntimeError: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label\n","  File \"<ipython-input-11-823d6085a09e>\", line 129, in train_loop\n","    for n_batch, image in enumerate(loader):\n","  File \"<ipython-input-12-673f955cbafa>\", line 5, in _mp_fn\n","    train(rank)\n","  File \"<ipython-input-11-823d6085a09e>\", line 139, in train\n","    loss = train_loop(pl.MpDeviceLoader(train_loader, device))\n","  File \"<ipython-input-11-823d6085a09e>\", line 129, in train_loop\n","    for n_batch, image in enumerate(loader):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 34, in __next__\n","    return self.next()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 34, in __next__\n","    return self.next()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 46, in next\n","    xm.mark_step()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 718, in mark_step\n","    wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 46, in next\n","    xm.mark_step()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 718, in mark_step\n","    wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))\n","RuntimeError: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label\n","RuntimeError: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label\n","Exception in device=TPU:4: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 329, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    fn(gindex, *args)\n","  File \"<ipython-input-12-673f955cbafa>\", line 5, in _mp_fn\n","    train(rank)\n","  File \"<ipython-input-11-823d6085a09e>\", line 139, in train\n","    loss = train_loop(pl.MpDeviceLoader(train_loader, device))\n","  File \"<ipython-input-11-823d6085a09e>\", line 129, in train_loop\n","    for n_batch, image in enumerate(loader):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 34, in __next__\n","    return self.next()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 46, in next\n","    xm.mark_step()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 718, in mark_step\n","    wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))\n","RuntimeError: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label\n","Exception in device=TPU:5: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 329, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    fn(gindex, *args)\n","  File \"<ipython-input-12-673f955cbafa>\", line 5, in _mp_fn\n","    train(rank)\n","  File \"<ipython-input-11-823d6085a09e>\", line 139, in train\n","    loss = train_loop(pl.MpDeviceLoader(train_loader, device))\n","  File \"<ipython-input-11-823d6085a09e>\", line 129, in train_loop\n","    for n_batch, image in enumerate(loader):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 34, in __next__\n","    return self.next()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/parallel_loader.py\", line 46, in next\n","    xm.mark_step()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 718, in mark_step\n","    wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))\n","RuntimeError: Resource exhausted: From /job:tpu_worker/replica:0/task:0:\n","2 root error(s) found.\n","  (0) Resource exhausted: Ran out of memory in memory space hbm. Used 8.04G of 7.98G hbm. Exceeded hbm capacity by 60.28M.\n","\n","Total hbm usage >= 8.06G:\n","    reserved         18.00M \n","    program           7.89G \n","    arguments       151.68M \n","\n","Output size 69.61M; shares 67.54M with arguments.\n","\n","Program hbm requirement 7.89G:\n","    global            36.0K\n","    scoped           732.0K\n","    HLO temp          7.71G (47.3% utilization: Unpadded (3.64G) Padded (7.70G), 0.2% fragmentation (12.77M))\n","    overlays        185.91M\n","\n","  Largest program allocations in hbm:\n","\n","  1. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3106 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p49.57, f32[8,3,3,3]{0,3,2,1:T(4,128)} %p50.58, bf16[4,3,512,1]{1,0,3,2:T(4,128)(2,1)} %get-tuple-element.7952, f32[4,3,512,512]{1,0,3,2:T(4,12...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  2. Size: 512.00M\n","     Shape: f32[4,512,512,40]{3,0,2,1:T(4,128)}\n","     Unpadded size: 160.00M\n","     Extra memory due to padding: 352.00M (3.2x expansion)\n","     XLA label: %custom-call.4 = f32[4,512,512,40]{3,0,2,1:T(4,128)} custom-call(f32[4,256,256,40]{3,0,2,1:T(4,128)} %copy.654), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  3. Size: 512.00M\n","     Shape: f32[4,8,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 480.00M (16.0x expansion)\n","     XLA label: %fusion.3104 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,512,512]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p37.45, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p38.46, f32[4,8,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8585, f32[8]{0:T(256)} %fusion.2665, f32[...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  4. Size: 512.00M\n","     Shape: f32[4,48,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 192.00M\n","     Extra memory due to padding: 320.00M (2.7x expansion)\n","     XLA label: %fusion.3101 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[4,48,512,512]{1,0,3,2:T(4,128)}) fusion(f32[48]{0:T(256)} %p15.23, f32[48,48,1,1]{1,0,3,2:T(8,128)} %p16.24, f32[4,48,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8612, f32[48]{0:T(256)} %fusion.26...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  5. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3099 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8614, f32[3]{0:T(256)} %fusion...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  6. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.3097 = (f32[64]{0:T(256)}, f32[64]{0:T(256)}, f32[4,64,512,512]{1,0,3,2:T(4,128)}) fusion(f32[64]{0:T(256)} %p265.1244, f32[64,3,3,3]{0,3,2,1:T(4,128)} %p258.1216, f32[4,3,512,512]{1,0,3,2:T(4,128)} %fusion.98, f32[3]{0:T(256)} %bitcast.94, f32[3]{...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  7. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.45.remat4 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8615, f32[64]{0:T(256)} %fusion.2690, f32[64]{0:T(256)} %p263.1...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  8. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.3.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.3.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  9. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %select-and-scatter.6.remat_uncompressed = f32[4,64,512,512]{1,0,3,2:T(4,128)} copy(f32[4,64,512,512]{3,2,1,0:T(8,128)} %select-and-scatter.6.remat_compressed)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  10. Size: 512.00M\n","     Shape: f32[4,64,512,512]{1,0,3,2:T(4,128)}\n","     Unpadded size: 256.00M\n","     Extra memory due to padding: 256.00M (2.0x expansion)\n","     XLA label: %fusion.44.remat7 = f32[4,64,512,512]{1,0,3,2:T(4,128)} fusion(f32[64,64,3,3]{1,0,3,2:T(8,128)} %p269.1315, f32[64]{0:T(256)} %p274.1320, f32[4,64,512,512]{1,0,3,2:T(4,128)} %get-tuple-element.8582, f32[64]{0:T(256)} %get-tuple-element.8175, f32[64]{0:T(25...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  11. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  12. Size: 256.00M\n","     Shape: bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}\n","     Unpadded size: 128.00M\n","     Extra memory due to padding: 128.00M (2.0x expansion)\n","     XLA label: %fusion.3195 = (f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}, f32[64]{0:T(256)}, bf16[4,64,512,512]{1,0,3,2:T(4,128)(2,1)}) fusion(f32[64]{0:T(256)} %get-tuple-element.7238, f32[64]{0:T(256)} %get-tuple-element.6919, f32[64]{0:T(256)} %get-...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  13. Size: 185.91M\n","     XLA label: overlays\n","     Allocation type: overlays\n","     ==========================\n","\n","  14. Size: 128.00M\n","     Shape: f32[4,8,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 8.00M\n","     Extra memory due to padding: 120.00M (16.0x expansion)\n","     XLA label: %fusion.3120 = (f32[8]{0:T(256)}, f32[8]{0:T(256)}, f32[4,8,256,256]{1,0,3,2:T(4,128)}) fusion(f32[8]{0:T(256)} %p84.208, f32[8,8,1,1]{1,0,3,2:T(8,128)} %p85.209, f32[4,8,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8588, f32[8]{0:T(256)} %fusion.2669, f3...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  15. Size: 128.00M\n","     Shape: f32[4,256,256,32]{3,0,2,1:T(4,128)}\n","     Unpadded size: 32.00M\n","     Extra memory due to padding: 96.00M (4.0x expansion)\n","     XLA label: %custom-call.3 = f32[4,256,256,32]{3,0,2,1:T(4,128)} custom-call(f32[4,128,128,32]{3,0,2,1:T(4,128)} %copy.648), custom_call_target=\"ResizeNearest\", backend_config=\"\\\"00\\\"\"\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  16. Size: 128.00M\n","     Shape: bf16[4,64,262144]{2,1,0:T(8,128)(2,1)}\n","     Unpadded size: 128.00M\n","     XLA label: %copy.660 = bf16[4,64,262144]{2,1,0:T(8,128)(2,1)} copy(bf16[4,64,262144]{1,0,2:T(4,128)(2,1)} %bitcast.54)\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  17. Size: 128.00M\n","     Shape: f32[4,40,256,256]{1,0,3,2:T(4,128)}\n","     Unpadded size: 40.00M\n","     Extra memory due to padding: 88.00M (3.2x expansion)\n","     XLA label: %fusion.390 = (f32[40]{0:T(256)}, f32[40]{0:T(256)}, f32[4,40,256,256]{1,0,3,2:T(4,128)}) fusion(f32[40]{0:T(256)} %p62.186, f32[40,40,1,1]{1,0,3,2:T(8,128)} %p63.187, f32[4,40,256,256]{1,0,3,2:T(4,128)} %get-tuple-element.8609, f32[40]{0:T(256)} %fusion.2...\n","     Allocation type: HLO temp\n","     ==========================\n","\n","  18. Size: 120.47M\n","     Shape: bf16[4,48,514,514]{3,1,2,0:T(8,128)(2,1)}\n","     Unpadded size: 96.75M\n","     Extra memory due to padding: 23.72M (1.2x expansion)\n","     XLA label\n"]},{"output_type":"error","ename":"ProcessExitedException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-673f955cbafa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_cores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0merror_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0merror_pid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     \u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 )\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mProcessExitedException\u001b[0m: process 6 terminated with exit code 17"]}]},{"cell_type":"code","metadata":{"id":"xFQgcXuFUJuR","executionInfo":{"status":"aborted","timestamp":1632364742192,"user_tz":420,"elapsed":95,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"}}},"source":[""],"execution_count":null,"outputs":[]}]}