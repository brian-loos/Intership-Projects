{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":779,"status":"ok","timestamp":1632438968237,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"},"user_tz":420},"id":"Kh3P1NMyxZzu"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","import torchvision.models as visionmodels\n","import torchvision.transforms as transforms\n","import torch.utils.data.dataloader as DataLoader\n","import torch.utils.data.dataset as Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable,grad\n","from torchvision import datasets\n","from collections import *\n","import os\n","from torchvision.io import read_image\n","from datetime import datetime\n","import glob\n","from zipfile import ZipFile\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28140,"status":"ok","timestamp":1632440637297,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"},"user_tz":420},"id":"lkkc8EeMxvMm","outputId":"b6eaa795-5392-425a-ad0a-118053749eb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","\n","FLAGS = {} \n","FLAGS['datadir'] = 'data/cats'\n","FLAGS['labeldir'] = '/content/gdrive/MyDrive/Colab Notebooks/data/cats_train.csv'\n","FLAGS['batch_size'] = 2\n","FLAGS['learning_rate'] = .0005\n","FLAGS['betas'] = (.5,.99)\n","FLAGS['num_epochs'] = 2000\n","FLAGS['im_channels'] = 3\n","\n","FLAGS['style_weight'] = 1.\n","FLAGS['content_weight'] = 1. \n","\n","FLAGS['im_size'] = 256\n","FLAGS['val_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats/val_cat.jpg' \n","FLAGS['style_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/monet/07.jpg' \n","FLAGS['val_size_mult'] = 2\n","FLAGS['output_path'] = 'outputs/ADAIN'\n","FLAGS['model_path'] = 'models/ADAIN'\n","FLAGS['output_fname'] = 'test_output'\n","FLAGS['model_fname'] = 'model'\n","FLAGS['home_dir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer'\n","\n","\n","zip_loc = '/content/gdrive/MyDrive/Colab Notebooks/data/cats.zip'\n","with ZipFile(zip_loc, 'r') as zf: \n","  zf.extractall('data/cats')\n","zip_loc_monet = '/content/gdrive/MyDrive/Colab Notebooks/data/monet_jpg.zip'\n","with ZipFile(zip_loc_monet, 'r') as zf: \n","  zf.extractall('data/monet') \n"]},{"cell_type":"markdown","metadata":{"id":"7_cuoPQvyAwW"},"source":["#Output and Saving Methods"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1632438995891,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"},"user_tz":420},"id":"blRzqaCdx9uq"},"outputs":[],"source":["def plot_image(test_image, val_image, save_result: bool = True): \n","  fig = plt.figure(figsize = (5,5), dpi = 200) \n","  ax = fig.subplots(2)\n","  ax[0].imshow(test_image[0].detach().cpu().permute(1,2,0))\n","  ax[1].imshow(val_image[0].detach().cpu().permute(1,2,0))\n","  if save_result: \n","    os.chdir(FLAGS['home_dir'])\n","    try: \n","      os.chdir(FLAGS['output_path']) \n","    except: \n","      for dir in FLAGS['output_path'].split('/'):\n","        try:\n","          os.chdir(dir)\n","        except: \n","          os.mkdir(dir) \n","          os.chdir(dir)\n","    fname = FLAGS['output_fname'] \n","    num_imgs = len(glob.glob('*.png')) \n","    fname += '_'+str(num_imgs)+'.png'\n","    plt.savefig(fname)\n","    plt.show(block = False)\n","    os.chdir('/content/') \n","  \n","def save_model(model): \n","  os.chdir(FLAGS['home_dir'])\n","  try: \n","    os.chdir(FLAGS['model_pathv']) \n","  except: \n","    for dir in FLAGS['model_path'].split('/'):\n","      try:\n","        os.chdir(dir)\n","      except: \n","        os.mkdir(dir) \n","        os.chdir(dir)\n","  fname = FLAGS['model_fname']\n","  num_models = len(glob.glob('*.model')) \n","  fname += '_'+str(num_models)+'.model'\n","  torch.save(model.state_dict(), fname)\n","  os.chdir('/content/')"]},{"cell_type":"markdown","metadata":{"id":"MMl8LfutyDJE"},"source":["#Models"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":279,"status":"ok","timestamp":1632440850795,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"},"user_tz":420},"id":"29iJjXhDyDLz"},"outputs":[],"source":["class Normalization(nn.Module): \n","  def __init__(self,mean,std): \n","    super().__init__() \n","    self.mean = mean.view(1,-1,1,1)\n","    self.std = std.view(1,-1,1,1) \n","  def forward(self,x):  \n","    return (x-self.mean)/self.std \n","\n","'''\n","  doens't use feature matched content\n","'''\n","class FeatureNetwork(nn.Module): \n","  def __init__(self,vgg_features): \n","    super().__init__()\n","    \n","    #feature layers of a vgg19_bn network\n","    #self.content_layers = [32]\n","    #self.style_layers = [2,9,26,23,30,42]\n","    \n","    #layers of vgg19 net\n","    #self.content_layers = [22] \n","    self.style_layers = [1,6,11,20]\n","    max_layer = self.style_layers[-1]\n","    #layers of vgg16\n","    # self.content_layers = [8] \n","    # self.style_layers = [3,8,15,18,24]\n","\n","    self.features = [feature.to(device) for i,feature in enumerate(vgg_features) if i \u003c=max_layer]\n","\n","    mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n","    std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n","    self.norm = Normalization(mean,std)\n","  def forward(self,input): \n","    input = self.norm(input)\n","    styles = [] \n","    for i,module in enumerate(self.features): \n","      input = module(input) \n","      if i in self.style_layers: \n","        styles.append(input)\n","    return  styles\n","\n","\n","\n","'''\n","  AdaIN layer\n","  Takes in a layers and aligns a target images styles to a style images mean and variance\n","  need to run this for each style\n","'''\n","class AdaIN(nn.Module): \n","  def __init__(self): \n","    super().__init__()  \n","    \n","  #given two feature maps --\u003e N x C x K x K \n","  #want to compute mean and var\n","  def forward(self, source_style, target_mean, target_dev): \n","    n,c,_,_ = source_style.shape\n","    mu_x = self.compute_mean(source_style)\n","    sigma_x = self.compute_var(source_style, mu_x)\n","    return (target_dev.view(n,c,1,1)*(source_style - mu_x)/sigma_x) + target_mean.view(n,c,1,1) \n","  \n","  #compute the instance mean of a feature tensor\n","  #returns an N x C x 1 x 1 tensor\n","  def compute_mean(self, style):\n","    n,c,h,w = style.shape \n","    mu = style.sum(dim = (2,3)) \n","    return mu.div(h*w).view(n,c,1,1) #---\u003e normalize mean \n","  \n","  #returns an N x C x 1 x 1 tensor\n","  def compute_var(self, style, mean, eps = 1e-8): \n","    n,c,h,w = style.shape \n","    sigma = style.sum(dim = (2,3)).div(h*w) + eps\n","    return sigma.sqrt().view(n,c,1,1)\n","\n","'''\n","  applies adain to each layer to normalize\n","  this output list now has transformed features and means and stds of origianl output\n","'''\n","class AdaModule(nn.Module): \n","  def __init__(self): \n","    super().__init__() \n","    self.adain = AdaIN()\n","  def forward(self, styles, target_means, target_devs): \n","    out = [] \n","    for style, target_mean, target_dev in zip(styles, target_means, target_devs): \n","      moduled_styles =self.adain(style,target_mean, target_dev)\n","      out += [moduled_styles]\n","    return out\n","'''\n","  decoder, mimics vgg network layout, inverted, with no normalization\n","'''\n","class VggConvBlock(nn.Module): \n","  def __init__(self, in_channels, out_channels,num_layers = 4, kernel = 3, stride =1 , padding = 1, padding_mode = 'reflect', bias = True): \n","    super().__init__() \n","    self.conv_layer = nn.Sequential(\n","        nn.Conv2d(in_channels, in_channels, kernel_size= kernel, stride = stride, padding = padding, padding_mode = padding_mode, bias = bias), \n","        nn.LeakyReLU(.2, inplace = False) \n","    )\n","    self.final_layer = nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size=kernel, stride = stride, padding = padding, padding_mode=padding_mode, bias = bias), \n","        nn.LeakyReLU(.2, inplace = False)\n","    )\n","    layers = [] \n","    for i in range(num_layers-1): \n","      layers += [self.conv_layer]\n","    layers += [self.final_layer]\n","    self.conv_block = nn.Sequential(*layers)\n","\n","  def forward(self,input,style = None):\n","    if style is not None: \n","      input = torch.cat((input,style), dim = 1)  \n","    return self.conv_block(input)\n","\n","class RealTimeDecoder(nn.Module): \n","  def __init__(self): \n","    super().__init__() \n","    self.module = nn.ModuleDict()\n","    upsample = nn.Upsample(scale_factor=2, mode = 'nearest')\n","    self.module['convblock1'] = VggConvBlock(512,256, num_layers = 4)\n","    self.module['convblock2'] = VggConvBlock(256*2,128, num_layers = 4)\n","    self.module['convblock3'] = VggConvBlock(128*2,64, num_layers = 2)\n","    self.module['convblock4'] = VggConvBlock(64*2,3, num_layers = 2)\n","    self.module['upsample'] = upsample\n","\n","  '''\n","    input is a list of styles of lengh 4\n","  '''\n","  def forward(self,input): \n","    out = self.module['convblock1'](input[0])\n","    out = self.module['upsample'](out)\n","    out = self.module['convblock2'](out,input[1])\n","    out = self.module['upsample'](out)\n","    out = self.module['convblock3'](out,input[2])\n","    out = self.module['upsample'](out)\n","    out = self.module['convblock4'](out,input[3])\n","    return out \n","    "]},{"cell_type":"markdown","metadata":{"id":"umFpSA5aCbOP"},"source":["#Losses"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":129,"status":"ok","timestamp":1632440851751,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"},"user_tz":420},"id":"K6MyJ1WU2RWb"},"outputs":[],"source":["'''\n","  Style Losses defined here do not rely on gram matrices but rather losses on statistics\n","'''\n","class StyleLoss(nn.Module): \n","  def __init__(self): \n","    super().__init__() \n","    \n","  def forward(self,input, means, stds): \n","    loss = 0\n","    for s, mean, std in zip(input, means,stds): \n","      loss += torch.linalg.norm(self.compute_mean(s)-mean, 2, 1) + torch.linalg.norm(self.compute_var(s,self.compute_mean(s))-std, 2, 1)\n","    return loss.mean()\n","\n","  def compute_mean(self, style):\n","    n,c,h,w = style.shape \n","    mu = style.sum(dim = (2,3)) \n","    return mu.div(h*w) #---\u003e normalize mean \n","  \n","  def compute_var(self, style, mean, eps = 1e-8): \n","    n,c,h,w = style.shape \n","    sigma = style.sum(dim =(2,3)).div(h*w) + eps\n","    return sigma.sqrt() \n","\n","\n","'''\n","  compute normalized MSELoss\n","'''\n","def normMSELoss(input,target): \n","  n, c, h, w = input.shape\n","  return (1/(c*h*w))*torch.linalg.norm(input.view(n,c,-1)-target.view(n,c,-1),2,2).sum()\n","\n","'''\n","  compute content loss between modulated features against generated image features\n","'''\n","def contentLoss(contents, target_content): \n","  content_loss = 0 \n","  for content,target in zip(contents, target_content): \n","    content_loss+= normMSELoss(content,target) \n","  return content_loss\n","\n","'''\n","  pixel loss and total variation loss from johnson\n","'''\n","def pixelLoss(gen_image, target_image): \n","  assert gen_image.shape == target_image.shape\n","  n,c,h,w = gen_image.shape\n","  return (1/(n*c*h*w))*torch.linalg.norm(gen_image.view(n,c,-1)-target_image.view(n,c,-1),2,2).sum()\n"]},{"cell_type":"markdown","metadata":{"id":"dqH5wqFdCZHo"},"source":["#Data Loader and initialization "]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1632440851890,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"},"user_tz":420},"id":"qR9liVSz2V4p"},"outputs":[],"source":["class customDataset(Dataset.Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None,target_transform = None,index = 2):\n","        '''\n","            we just want an unlabelled image dataset\n","        '''\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.ind =index\n","\n","    def __len__(self):\n","        return (len(self.img_labels))\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, self.ind])\n","        image = read_image(img_path)/255\n","        if self.transform:\n","            image = self.transform(image)\n","        return image\n","\n","def _initialize_dataset(data_path = None, label_path = None, data_transform = None,target_transform = None,index = 2):\n","    dataset = customDataset(label_path, data_path, transform=data_transform,target_transform = target_transform,index = index)\n","    training_set = torch.utils.data.DataLoader(\n","         dataset, batch_size=FLAGS['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n","    return training_set,dataset\n","\n","def init_weights(m):\n","  if isinstance(m,nn.Conv2d):\n","    torch.nn.init.xavier_normal_(m.weight)\n","    if m.bias.data is not None: \n","      m.bias.data.fill_(0.)\n","  elif isinstance(m,nn.ConvTranspose2d): \n","    torch.nn.init.xavier_normal_(m.weight)\n","    if m.bias.data is not None: \n","      m.bias.data.fill_(0.)"]},{"cell_type":"markdown","metadata":{"id":"2OGfO7g6I7hD"},"source":["#Training method "]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1632440851890,"user":{"displayName":"Brian Loos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1Omx-gtNjQ6v6f5Tc-TQY3g3YeuHXokqClLTgmA=s64","userId":"13749232592335626235"},"user_tz":420},"id":"R8dgBXAwI7kV"},"outputs":[],"source":["def train(feature_net, \n","          generator, \n","          ada_module,\n","          optimizer, \n","          scheduler, \n","          loader, \n","          styleLoader,\n","          val_image, \n","          style_image\n","          ):\n","  for p in feature_net.parameters(): \n","    p.requires_grad = False\n","  style_input = Variable(style_image,requires_grad = False).to(device)\n","  val_input = Variable(val_image, requires_grad = False).to(device)\n","  val_styles = feature_net(style_input) \n","  \n","  target_styles = feature_net(style_input)\n","   \n","  styleloss = StyleLoss() \n","  \n"," \n","  '''\n","    create set of targetstyles\n","  '''\n","  num_target_styles_batches = 50\n","  means = [] \n","  devs = [] \n","  for i,style_image in enumerate(styleLoader): \n","      input = Variable(style_image).to(device)\n","      target_styles = feature_net(input)\n","      if i \u003e= num_target_styles_batches: \n","        break\n","      m = [] \n","      d = [] \n","      for s in target_styles: \n","        m +=[styleloss.compute_mean(s).detach().cpu()]\n","        d += [styleloss.compute_var(s,m[-1]).detach().cpu()] \n","      means += [m] \n","      devs += [d]\n","  r = np.random.randint(0,num_target_styles_batches-1)\n","  test_means = [m[0].cuda() for m in means[:][r]]\n","  test_devs = [d[0].cuda() for d in devs[:][r]]\n","  val_mod_styles = ada_module(val_styles, test_means, test_devs)[::-1]\n","\n","  for epoch in range(FLAGS['num_epochs']): \n","    for p in generator.parameters(): \n","      p.requires_grad = True\n","    print('processing epoch {}/{}'.format(epoch+1, FLAGS['num_epochs']))\n","    generator.train()\n","    #generate random target styles\n","    \n","    for i,image in enumerate(loader): \n","      batch_size = len(image)\n","      r = np.random.randint(0,num_target_styles_batches-1) \n","      target_means = [m.cuda() for m in means[:][r]] \n","      \n","      target_devs = [d.cuda() for d in devs[:][r]]\n","      if batch_size != FLAGS['batch_size']: \n","        break\n","     \n","      input = Variable(image).to(device)  \n","      \n","      '''\n","          forward pass\n","          take images and pass through encoder, no gradient for this part\n","      '''\n","      with torch.no_grad(): \n","        input_styles = feature_net(input)\n","        '''\n","          use input styles and align to target styles \n","        '''\n","        modulated_styles = ada_module(input_styles, target_means, target_devs)[::-1]\n","      \n","      '''\n","        pass through the decoder network\n","      '''\n","      optimizer.zero_grad() \n","      generated_output = generator(modulated_styles)\n","      '''\n","        compute styles after generation\n","      '''\n","      styles = feature_net(generated_output)\n","      #loss against the style image statistics\n","      #currently causes an error\n","      style_loss = styleloss(styles, target_means, target_devs)*FLAGS['style_weight']\n","      style_loss.backward(retain_graph = True)\n","      #loss against the feature maps\n","      content_loss = contentLoss(styles[::-1], modulated_styles) *FLAGS['content_weight']\n","      content_loss.backward()\n","      \n","      optimizer.step() \n","      if (i+1)%10 == 0: \n","        print('current style loss: {}, content loss: {}'.format(style_loss.item(),content_loss.item()))\n","\n","    scheduler.step()\n","     \n","    #print('stats:{}'.format(loss.detach()))\n","  \n","    generator.eval()\n","    for p in generator.parameters(): \n","      p.requires_grad = False\n","    test_out = generator(val_mod_styles)\n","    plot_image(generated_output, test_out)\n","\n","  \n","    save_model(generator)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1TCdePgKYvHNpLJNFUJuXa0Zcfffak7Mf"},"id":"SFSkOMtBI-2W","outputId":"7fac83ce-bbc3-4893-af9c-178b86b3f087"},"outputs":[],"source":["if __name__ == '__main__': \n","  os.chdir('/content/')\n","  %ls \n","  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","  vgg_net = visionmodels.vgg16(pretrained=True)\n","  vgg_net.eval()\n","  feature_net = FeatureNetwork(list(vgg_net.features)).to(device) \n","  ada_module = AdaModule().to(device)\n","  generator = RealTimeDecoder().to(device)\n","  generator.apply(init_weights)\n","  optimizer = optim.Adam(generator.parameters(), lr = 0.00005, betas = FLAGS['betas'])\n","  #optimizer = optim.LBFGS(generator.parameters()) \n","  def lmbda(epoch): \n","    if epoch == 1000: \n","      return .1 \n","    if epoch \u003e 1000: \n","      if epoch %200 == 0: \n","        return .1\n","      else: \n","        return 1 \n","    else: \n","      return 1\n","  scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda = lmbda)\n","\n","  im_size = FLAGS['im_size']\n","  test_mult = FLAGS['val_size_mult']\n","\n","  transform = transforms.Compose([ transforms.Resize(im_size*2), transforms.RandomCrop((im_size,im_size))])\n","  val_transform = transforms.Compose([transforms.Resize(im_size*test_mult*2),transforms.RandomCrop((test_mult*im_size,test_mult*im_size))])\n","\n","  train_loader,dataset = _initialize_dataset(data_path = FLAGS['datadir'], label_path = FLAGS['labeldir'],\\\n","                                data_transform = transform)\n","  \n","  monet_loader,dataset = _initialize_dataset(data_path = 'data/monet', label_path = FLAGS['labeldir'],\\\n","                                data_transform = transform,index = 3)\n","  \n","  val_image = val_transform(read_image(FLAGS['val_image_path'])/255).unsqueeze(0)\n","  style_image = transform(read_image(FLAGS['style_image_path'])/255).unsqueeze(0)\n","\n","  output = train(feature_net,       #network for extracting features\n","                  generator, \n","                  ada_module, \n","                  optimizer, \n","                  scheduler, \n","                  train_loader,\n","                  monet_loader, \n","                  val_image,                    #content target image\n","                  style_image                      #style target image\n","                  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RDEi7FYOc9H"},"outputs":[],"source":["%cd \n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGCLwdWZhpb4"},"outputs":[],"source":["%cd /content/data\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9REoQRRhsX2"},"outputs":[],"source":["%cd cats \n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lc8uxGF-hvvp"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOhp8fPCcgfusCgQ3qY5zaA","collapsed_sections":[],"name":"Real Time Arbitrary Style Transfer - AdaIN.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}