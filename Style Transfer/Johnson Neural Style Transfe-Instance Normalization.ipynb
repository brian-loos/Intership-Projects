{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Johnson Neural Style Transfe-Instance Normalization.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["9PGgWuj8Wq8u"],"authorship_tag":"ABX9TyNgOMCTXbF7gnXFBWBCWdVD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"UKF-RGIKLuih"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","import torchvision.models as visionmodels\n","import torchvision.transforms as transforms\n","import torch.utils.data.dataloader as DataLoader\n","import torch.utils.data.dataset as Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable,grad\n","from torchvision import datasets\n","from collections import *\n","import os\n","from torchvision.io import read_image\n","from datetime import datetime\n","import glob\n","from zipfile import ZipFile\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k7W3O4DhMExR"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","\n","FLAGS = {} \n","FLAGS['datadir'] = 'data/cats'\n","FLAGS['labeldir'] = '/content/gdrive/MyDrive/Colab Notebooks/data/cats_small.csv'\n","FLAGS['batch_size'] = 4\n","FLAGS['learning_rate'] = .001\n","FLAGS['num_epochs'] = 2000\n","FLAGS['im_channels'] = 3\n","FLAGS['noise_scale_factor'] = .01\n","\n","FLAGS['style_weight'] = 1.\n","FLAGS['content_weight'] = 1. \n","FLAGS['tv_weight'] = 1e-4\n","FLAGS['pixel_weight'] = 10. \n","\n","FLAGS['im_size'] = 256\n","FLAGS['val_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats/val_cat.jpg' \n","FLAGS['style_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/monet/07.jpg' \n","FLAGS['val_size_mult'] = 2\n","FLAGS['output_path'] = 'outputs/Johnson_NST_IN'\n","FLAGS['model_path'] = 'models/Johnson_NST_IN'\n","FLAGS['output_fname'] = 'test_output'\n","FLAGS['model_fname'] = 'model'\n","FLAGS['home_dir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer'\n","\n","\n","zip_loc = '/content/gdrive/MyDrive/Colab Notebooks/data/cats_small.zip'\n","with ZipFile(zip_loc, 'r') as zf: \n","  zf.extractall('data/cats')\n","\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9PGgWuj8Wq8u"},"source":["#Output and Saving Methods"]},{"cell_type":"code","metadata":{"id":"GZbxllruVyfL"},"source":["def plot_image(test_image, val_image, save_result: bool = True): \n","  fig = plt.figure(figsize = (5,5), dpi = 200) \n","  ax = fig.subplots(2)\n","  ax[0].imshow(test_image[0].detach().cpu().permute(1,2,0))\n","  ax[1].imshow(val_image[0].detach().cpu().permute(1,2,0))\n","  if save_result: \n","    os.chdir(FLAGS['home_dir'])\n","    try: \n","      os.chdir(FLAGS['output_path']) \n","    except: \n","      for dir in FLAGS['output_path'].split('/'):\n","        try:\n","          os.chdir(dir)\n","        except: \n","          os.mkdir(dir) \n","          os.chdir(dir)\n","    fname = FLAGS['output_fname'] \n","    num_imgs = len(glob.glob('*.png')) \n","    fname += '_'+str(num_imgs)+'.png'\n","    plt.savefig(fname)\n","    plt.show(block = False)\n","    os.chdir('/content/') \n","  \n","def save_model(model): \n","  os.chdir(FLAGS['home_dir'])\n","  try: \n","    os.chdir(FLAGS['model_pathv']) \n","  except: \n","    for dir in FLAGS['model_path'].split('/'):\n","      try:\n","        os.chdir(dir)\n","      except: \n","        os.mkdir(dir) \n","        os.chdir(dir)\n","  fname = FLAGS['model_fname']\n","  num_models = len(glob.glob('*.model')) \n","  fname += '_'+str(num_models)+'.model'\n","  torch.save(model.state_dict(), fname)\n","  os.chdir('/content/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yD_j2CenWpMB"},"source":["#Models"]},{"cell_type":"code","metadata":{"id":"67JlA_iWPcE0"},"source":["'''\n","  single convolution layers\n","'''\n","class JohnsonConvLayer(nn.Module): \n","  def __init__(self, in_channels, out_channels,kernel: int = 3, stride : int = 1, padding:int=1, output_padding:int = 0, padding_mode:str = 'reflect',bias : bool = True, upsample: bool = False, downsample: bool = False,relu_inplace: bool = True): \n","    super().__init__() \n","    layers = [] \n","    if upsample:\n","      assert stride > 1\n","      layers += [nn.ConvTranspose2d(in_channels, out_channels,kernel_size = kernel, stride = stride, padding = padding, output_padding= output_padding,bias = bias, padding_mode = 'zeros')]\n","    if downsample:     \n","      assert stride > 1\n","      layers += [nn.Conv2d(in_channels, out_channels, kernel_size = kernel, stride = stride, padding = padding, padding_mode = padding_mode, bias = bias)]\n","    if (not upsample) and (not downsample): \n","      assert stride == 1\n","      layers += [nn.Conv2d(in_channels, out_channels, kernel_size = kernel, stride = stride, padding = padding, padding_mode = padding_mode, bias = bias)]\n","    layers += [nn.InstanceNorm2d(out_channels, affine = True)]\n","    layers += [nn.LeakyReLU(.2,inplace = relu_inplace)]\n","    assert len(layers) == 3\n","    self.conv_layer = nn.Sequential(*layers)  \n","  \n","  def forward(self,input): \n","    return self.conv_layer(input)\n","\n","'''\n","  residual layer\n","'''\n","class GrossResidualLayer(nn.Module): \n","  def __init__(self, in_channels, out_channels,kernel: int = 1, stride : int = 1, padding:int=1, padding_mode:str = 'reflect',bias : bool = True): \n","    super().__init__() \n","    self.conv_layer = nn.Sequential(\n","        nn.Conv2d(in_channels,out_channels, kernel_size = kernel, stride = stride, padding = padding, padding_mode = padding_mode, bias = bias), \n","        nn.InstanceNorm2d(out_channels, affine = True), \n","        nn.LeakyReLU(.2,inplace = True), \n","        nn.Conv2d(out_channels,out_channels, kernel_size = kernel, stride = stride, padding = padding, padding_mode = padding_mode, bias = bias), \n","        nn.InstanceNorm2d(out_channels, affine = True)\n","    )\n","\n","  def forward(self,input): \n","    return input + self.conv_layer(input) \n","\n","'''\n","  main generator network\n","'''\n","\n","class JohnsonGeneratorNetwork(nn.Module): \n","  def __init__(self): \n","    super().__init__() \n","    layers = [] \n","    layers += [JohnsonConvLayer(3,32,kernel = 9, stride = 1, padding = 4)] \n","    layers += [JohnsonConvLayer(32,64,kernel = 3, stride = 2, downsample = True)]\n","    layers += [JohnsonConvLayer(64,128,kernel  = 3, stride = 2, downsample = True)]\n","\n","    for i in range(5): \n","      layers += [GrossResidualLayer(128,128, kernel = 3, stride = 1)]\n","    \n","    layers += [JohnsonConvLayer(128,64,kernel = 3, stride = 2, padding = 0, output_padding = 1, upsample = True)]\n","    layers += [JohnsonConvLayer(64,32,kernel = 3, stride = 2, padding = 0, output_padding = 1, upsample = True)]\n","    layers += [JohnsonConvLayer(32,3,kernel = 9, stride = 1, padding = 1,relu_inplace= False)]\n","\n","    self.main = nn.Sequential(*layers)\n","\n","  def forward(self,input): \n","    return self.main(input) \n","\n","class Normalization(nn.Module): \n","  def __init__(self,mean,std): \n","    super().__init__() \n","    self.mean = mean.view(1,-1,1,1)\n","    self.std = std.view(1,-1,1,1) \n","  def forward(self,x):  \n","    return (x-self.mean)/self.std \n","\n","class FeatureNetwork(nn.Module): \n","  def __init__(self,vgg_features): \n","    super().__init__()\n","    self.features = [feature.to(device) for feature in vgg_features]\n","    #feature layers of a vgg19_bn network\n","    #self.content_layers = [32]\n","    #self.style_layers = [2,9,26,23,30,42]\n","    \n","    #layers of vgg19 net\n","    # self.content_layers = [22] \n","    # self.style_layers = [1,6,11,20,29]\n","\n","    #layers of vgg16\n","    self.content_layers = [8] \n","    self.style_layers = [3,8,15,18,24]\n","    mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n","    std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n","    self.norm = Normalization(mean,std)\n","  def forward(self,input): \n","    input = self.norm(input)\n","    content = []\n","    styles = [] \n","    for i,module in enumerate(self.features): \n","      input = module(input) \n","      if i in self.content_layers: \n","        content.append(input)\n","      elif i in self.style_layers: \n","        styles.append(input)\n","    return content, styles"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ctw4sAt4Wnf-"},"source":["# Losses"]},{"cell_type":"code","metadata":{"id":"bKMTxhXWWnDO"},"source":["def gram_matrix(A): \n","  N,C, h, w = A.shape\n","  if N == 1:\n","    A = A.view(C,-1)\n","    G = torch.mm(A,A.t()).view(1,C,-1) \n","  else: \n","    A = A.view(N,C,-1)\n","    G = torch.bmm(A,A.transpose(1,2))\n","  return G.div(C*h*w) #returns CxC normalized gram matrix\n","\n","def gramMSELoss(input,target): \n","  #assuming target is already a gram matrix \n","  G = gram_matrix(input)\n","  return F.mse_loss(G,target) \n","\n","def gramFrobLoss(input,target): \n","  G = gram_matrix(input) \n","  return torch.linalg.norm(G-target,'fro',(1,2)).sum()\n","\n","def styleLoss(styles,target_styles,mode :str = 'fro'): \n","  style_loss = 0 \n","  for style,target in zip(styles,target_styles): \n","    if mode == 'fro': \n","      style_loss += gramFrobLoss(style,target) \n","    elif mode == 'mse': \n","      style_loss += gramMSELoss(style,target) \n","  return style_loss\n","\n","'''\n","  compute normalized MSELoss\n","'''\n","def normMSELoss(input,target): \n","  _, c, h, w = input.shape\n","  return (1/(c*h*w))*torch.linalg.norm(input-target,2,(2,3)).sum()\n","\n","'''\n","  compute content loss \n","'''\n","def contentLoss(contents, target_content): \n","  content_loss = 0 \n","  for content,target in zip(contents, target_content): \n","    content_loss+= normMSELoss(content,target) \n","  return content_loss\n","\n","\n","'''\n","  pixel loss and total variation loss from johnson\n","'''\n","def pixelLoss(gen_image, target_image): \n","  assert gen_image.shape == target_image.shape\n","  n,c,h,w = gen_image.shape\n","  return (1/(n*c*h*w))*torch.linalg.norm(gen_image.view(n,c,-1)-target_image.view(n,c,-1),2,2).sum()\n","\n","def totalVariationLoss(image): \n","  n, c, h, w = image.shape\n","  z = F.pad(image,(0,1,0,1))\n","  tv_reg = ((z[:,:,1:,:-1]-z[:,:,:-1,:-1]).pow(2) + (z[:,:,:-1,1:] - z[:,:,:-1,:-1]).pow(2)).sqrt().sum()\n","  return tv_reg/(n*c*h*w) \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Eqi4kjTdhfn"},"source":["#DataLoader and weight initialization"]},{"cell_type":"code","metadata":{"id":"du0avx13dhkp"},"source":["class customDataset(Dataset.Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None,target_transform = None):\n","        '''\n","            we just want an unlabelled image dataset\n","        '''\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return (len(self.img_labels))\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n","        image = read_image(img_path)/255\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        return image\n","\n","def _initialize_dataset(data_path = None, label_path = None, data_transform = None,target_transform = None):\n","    dataset = customDataset(label_path, data_path, transform=data_transform,target_transform = target_transform)\n","    training_set = torch.utils.data.DataLoader(\n","         dataset, batch_size=FLAGS['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n","    return training_set,dataset\n","\n","def init_weights(m):\n","  if isinstance(m,nn.Conv2d):\n","    torch.nn.init.xavier_normal_(m.weight)\n","    if m.bias.data is not None: \n","      m.bias.data.fill_(0.)\n","  elif isinstance(m,nn.ConvTranspose2d): \n","    torch.nn.init.xavier_normal_(m.weight)\n","    if m.bias.data is not None: \n","      m.bias.data.fill_(0.)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nkt7BuW1dWBU"},"source":["#Training Loop"]},{"cell_type":"code","metadata":{"id":"FgsHc0DgdWGC"},"source":["def train(feature_net, \n","          generator, \n","          optimizer, \n","          scheduler, \n","          loader, \n","          val_image, \n","          style_image\n","          ):\n","  for p in feature_net.parameters(): \n","    p.requires_grad = False\n","  style_input = Variable(style_image,requires_grad = False).to(device)\n","  val_input = Variable(val_image, requires_grad = False).to(device)\n","  _,target_styles = feature_net(style_input)\n","  fixed_noise = torch.rand(val_input.shape[0],FLAGS['im_channels'], FLAGS['im_size']*FLAGS['val_size_mult'], FLAGS['im_size']*FLAGS['val_size_mult'],device = device)*FLAGS['noise_scale_factor']\n","  val_input += fixed_noise \n","  val_input.clip_(0,1)\n","  for i,A in enumerate(target_styles): \n","    target_styles[i] = gram_matrix(A.detach()).tile(FLAGS['batch_size'],1,1)\n","\n","  for epoch in range(FLAGS['num_epochs']): \n","    for p in generator.parameters(): \n","      p.requires_grad = True\n","    print('processing epoch {}/{}'.format(epoch+1, FLAGS['num_epochs']))\n","    for i,image in enumerate(loader): \n","      batch_size = len(image)\n","      if batch_size != FLAGS['batch_size']: \n","        break\n","     \n","      input = Variable(image).to(device)  \n","      nz = Variable(torch.rand(FLAGS['batch_size'],FLAGS['im_channels'],FLAGS['im_size'],FLAGS['im_size'],device = device),requires_grad = False)*FLAGS['noise_scale_factor']\n","      input += nz \n","      input.clip_(0,1)\n","      optimizer.zero_grad() \n","      '''\n","          forward pass\n","      '''\n","      gen_output = generator(input)\n","      \n","      gen_output.clip_(0,1)\n","      '''\n","        generate test style and content \n","      '''\n","      contents,styles = feature_net(gen_output) \n","      '''\n","        generate actual content\n","      '''\n","      target_content, _  = feature_net(input)\n","      loss = 0 \n","\n","      loss += styleLoss(styles, target_styles)*FLAGS['style_weight']\n","      loss += contentLoss(contents, target_content) *FLAGS['content_weight']\n","      loss += pixelLoss(gen_output, input) *FLAGS['pixel_weight']\n","      loss += totalVariationLoss(gen_output) *FLAGS['tv_weight']\n","      loss.backward()\n","      \n","      optimizer.step() \n","    scheduler.step()\n","    if (epoch+1) % 10 == 0: \n","      print('stats:{}'.format(loss.detach()))\n","    if (epoch+1) % 10 == 0: \n","      for p in generator.parameters(): \n","        p.requires_grad = False\n","      test_out = generator(val_input)\n","      plot_image(gen_output, test_out)\n","\n","    if (epoch+1)%10 == 0: \n","      save_model(generator)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aiK4pa4TdlFE"},"source":["if __name__ == '__main__': \n","  os.chdir('/content/')\n","  %ls \n","  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","  vgg_net = visionmodels.vgg16(pretrained=True)\n","  vgg_net.eval()\n","  feature_net = FeatureNetwork(list(vgg_net.features)).to(device) \n","  generator = JohnsonGeneratorNetwork().to(device)\n","  generator.apply(init_weights)\n","  optimizer = optim.Adam(generator.parameters(), lr = FLAGS['learning_rate'], betas = (.5,.999))\n","  #optimizer = optim.LBFGS(generator.parameters()) \n","  def lmbda(epoch): \n","    if epoch == 1000: \n","      return .1 \n","    if epoch > 1000: \n","      if epoch %200 == 0: \n","        return .1\n","      else: \n","        return 1 \n","    else: \n","      return 1\n","  scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda = lmbda)\n","\n","  im_size = FLAGS['im_size']\n","  test_mult = FLAGS['val_size_mult']\n","\n","  transform = transforms.Compose([ transforms.Resize(im_size), transforms.CenterCrop((im_size,im_size))])\n","  val_transform = transforms.Compose([transforms.Resize(im_size*test_mult),transforms.CenterCrop((test_mult*im_size,test_mult*im_size))])\n","\n","  train_loader,dataset = _initialize_dataset(data_path = FLAGS['datadir'], label_path = FLAGS['labeldir'],\\\n","                                data_transform = transform)\n","  \n","  val_image = val_transform(read_image(FLAGS['val_image_path'])/255).unsqueeze(0)\n","  style_image = transform(read_image(FLAGS['style_image_path'])/255).unsqueeze(0)\n","\n","  output = train(feature_net,       #network for extracting features\n","                  generator, \n","                  optimizer, \n","                  scheduler, \n","                  train_loader,\n","                  val_image,                    #content target image\n","                  style_image                      #style target image\n","                  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dfoq-rQcf5x9"},"source":[""],"execution_count":null,"outputs":[]}]}