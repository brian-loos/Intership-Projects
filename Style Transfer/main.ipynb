{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main",
      "provenance": [],
      "collapsed_sections": [
        "_OM_P3H4jxkW"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNAo1Di7Bf2V8YHyQS992mn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brian-loos/Intership-Projects/blob/main/Style%20Transfer/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG3NbbphE7-v"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugs_IL3LgvMr"
      },
      "source": [
        "#!pip install --quiet \"pytorch-lightning>=1.3\" \"torchmetrics>=0.3\" \"torch>=1.6, <1.9\" \"torchvision\"\n",
        "#!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install pytorch-lightning\n",
        "!pip install pickle5\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torchvision\n",
        "import torchvision.models as visionmodels\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data.dataloader as DataLoader\n",
        "import torch.utils.data.dataset as Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable,grad\n",
        "from torchvision import datasets\n",
        "from collections import *\n",
        "import os\n",
        "from torchvision.io import read_image\n",
        "from datetime import datetime\n",
        "import glob\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount= True)\n",
        "pwd = '/content/'\n",
        "base_dir = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer'\n",
        "os.chdir(base_dir) \n",
        "import models \n",
        "os.chdir(pwd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU8rLUrsE-gK"
      },
      "source": [
        "#Globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0BR4FzhhPip"
      },
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "FLAGS = {} \n",
        "FLAGS['datadir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats'\n",
        "#FLAGS['labeldir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/landscapes.csv'\n",
        "FLAGS['labeldir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats_small.csv'\n",
        "FLAGS['styledir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/Paintings'\n",
        "FLAGS['styleanno'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/paintings.csv'\n",
        "FLAGS['batch_size'] = 16\n",
        "FLAGS['test_batch_size'] = 16\n",
        "FLAGS['learning_rate'] = .0005\n",
        "FLAGS['num_epochs'] = 2000\n",
        "FLAGS['im_channels'] = 3\n",
        "FLAGS['noise_scale_factor'] = .01\n",
        "FLAGS['style_weight'] = 2e5\n",
        "FLAGS['pixel_weight'] = 10. \n",
        "FLAGS['content_weight'] = 1.\n",
        "FLAGS['tv_weight'] = 1. \n",
        "FLAGS['im_size'] = 256\n",
        "FLAGS['val_image_path'] = ['/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats/cat-2083492_1280 - Copy (2).jpg',\n",
        "                           '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats/3683 - Copy (2).jpg',\n",
        "                           '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats/cat-read-to-pounce-julie-austin-photography - Copy (2).jpg',\n",
        "                           '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/cats/cat1.jpg'\n",
        "                            ]\n",
        "FLAGS['style_image_path'] = ['/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/Paintings/man_with_hat.jpg',\n",
        "                             '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/Paintings/wave.jpg',\n",
        "                             '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/Paintings/circle_geometric.jpg',\n",
        "                             '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/Paintings/strokes.jpg']\n",
        "FLAGS['val_size_mult'] = 2\n",
        "FLAGS['output_path'] = 'outputs/Texture_net_BN'\n",
        "FLAGS['model_path'] = 'models/Texture_net_BN'\n",
        "FLAGS['output_fname'] = 'test_output_'\n",
        "FLAGS['model_fname'] = 'landscape_model'\n",
        "FLAGS['home_dir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OM_P3H4jxkW"
      },
      "source": [
        "#Shortcuts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWR-MIgaiTrh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "bb6dcfda-9a4c-4a11-9321-a354615e08fa"
      },
      "source": [
        "'''\n",
        "  LITDataModule(train_batch_size = , \n",
        "                val_batch_size = ,\n",
        "                train_im_size = , \n",
        "                val_im_size = , \n",
        "                data_zip = [], \n",
        "                data_path = [], \n",
        "                train_anno_path = , \n",
        "                test_anno_path = , \n",
        "                val_anno_path = \n",
        "                )\n",
        "  LITAdaINDatamodule(train_batch_size = , \n",
        "                val_batch_size = ,\n",
        "                train_im_size = , \n",
        "                val_im_size = , \n",
        "                data_zip = [], \n",
        "                data_path = [], \n",
        "                train_anno_path = , \n",
        "                test_anno_path = , \n",
        "                val_anno_path = \n",
        "                )\n",
        "  LITTextureNet(style_image : torch.Tensor,\n",
        "                      batch_size : int = 8,\n",
        "                      normalization_method: str = 'BatchNorm2d', \n",
        "                      num_layers : int = 6, \n",
        "                      base_feat_maps: int = 8, \n",
        "                      im_channels: int = 3,\n",
        "                      vgg_net : str = 'vgg19', \n",
        "                      optimizer : str = 'Adam', \n",
        "                      learning_rate : float = .1, \n",
        "                      betas : tuple = (.5,.99),\n",
        "                      noise_scale_factor = .01,\n",
        "                      style_weight = 1e5, \n",
        "                      content_weight = 1.)\n",
        "  LITFastNST(style_image : torch.Tensor,\n",
        "                      batch_size : int = 8,\n",
        "                      normalization_method: str = 'BatchNorm2d',\n",
        "                      vgg_net : str = 'vgg19', \n",
        "                      optimizer : str = 'Adam', \n",
        "                      learning_rate : float = .1, \n",
        "                      betas : tuple = (.5,.99),\n",
        "                      noise_scale_factor = .01,\n",
        "                      style_weight = 1., \n",
        "                      content_weight = 1., \n",
        "                      pixel_weight = 10., \n",
        "                      tv_weight = 1.)\n",
        "  LITAdaIN(batch_size : int = 8,\n",
        "                      vgg_net : str = 'vgg19', \n",
        "                      optimizer : str = 'Adam', \n",
        "                      learning_rate : float = .1, \n",
        "                      betas : tuple = (.5,.99),\n",
        "                      noise_scale_factor = .01,\n",
        "                      style_weight = 1., \n",
        "                      content_weight = 1. )\n",
        "    \n",
        "  extract_features(data_dir,\n",
        "                  output_fname,\n",
        "                  vgg_net = \n",
        "                  im_size =   \n",
        "                  )\n",
        "  \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n  LITDataModule(train_batch_size = , \\n                val_batch_size = ,\\n                train_im_size = , \\n                val_im_size = , \\n                data_zip = [], \\n                data_path = [], \\n                train_anno_path = , \\n                test_anno_path = , \\n                val_anno_path = \\n                )\\n  LITAdaINDatamodule(train_batch_size = , \\n                val_batch_size = ,\\n                train_im_size = , \\n                val_im_size = , \\n                data_zip = [], \\n                data_path = [], \\n                train_anno_path = , \\n                test_anno_path = , \\n                val_anno_path = \\n                )\\n  LITTextureNet(style_image : torch.Tensor,\\n                      batch_size : int = 8,\\n                      normalization_method: str = 'BatchNorm2d', \\n                      num_layers : int = 6, \\n                      base_feat_maps: int = 8, \\n                      im_channels: int = 3,\\n                      vgg_net : str = 'vgg19', \\n                      optimizer : str = 'Adam', \\n                      learning_rate : float = .1, \\n                      betas : tuple = (.5,.99),\\n                      noise_scale_factor = .01,\\n                      style_weight = 1e5, \\n                      content_weight = 1.)\\n  LITFastNST(style_image : torch.Tensor,\\n                      batch_size : int = 8,\\n                      normalization_method: str = 'BatchNorm2d',\\n                      vgg_net : str = 'vgg19', \\n                      optimizer : str = 'Adam', \\n                      learning_rate : float = .1, \\n                      betas : tuple = (.5,.99),\\n                      noise_scale_factor = .01,\\n                      style_weight = 1., \\n                      content_weight = 1., \\n                      pixel_weight = 10., \\n                      tv_weight = 1.)\\n  LITAdaIN(batch_size : int = 8,\\n                      vgg_net : str = 'vgg19', \\n                      optimizer : str = 'Adam', \\n                      learning_rate : float = .1, \\n                      betas : tuple = (.5,.99),\\n                      noise_scale_factor = .01,\\n                      style_weight = 1., \\n                      content_weight = 1. )\\n    \\n  extract_features(data_dir,\\n                  output_fname,\\n                  vgg_net = \\n                  im_size =   \\n                  )\\n  \\n\""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "394QkAMbjyxN"
      },
      "source": [
        "#Test Area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0cSudZwFBse"
      },
      "source": [
        "##Unzip Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-X3dW-szd05"
      },
      "source": [
        "# models.unzip_data(['data/cats','data/monet'],['/content/gdrive/MyDrive/Colab Notebooks/data/cats.zip', '/content/gdrive/MyDrive/Colab Notebooks/data/monet_jpg.zip'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nob7wVEiFDbw"
      },
      "source": [
        "##Import data module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFlyGhfqk0wo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6da0ebe4-0090-4a53-a206-533c9e62f348"
      },
      "source": [
        "'''\n",
        "  Load Data Module for every other Module: Loads single style and multiple examples to train on \n",
        "'''\n",
        "dm = models.LITDataModule(train_batch_size = FLAGS['batch_size'] , \n",
        "                val_batch_size = FLAGS['test_batch_size'],\n",
        "                train_im_size = FLAGS['im_size'], \n",
        "                val_im_size = FLAGS['im_size']*FLAGS['val_size_mult'], \n",
        "                data_zip = [], \n",
        "                data_path = [FLAGS['datadir'],FLAGS['datadir']], \n",
        "                train_anno_path = FLAGS['labeldir'], \n",
        "                test_anno_path = FLAGS['labeldir'], \n",
        "                val_anno_path = FLAGS['labeldir']\n",
        "                )\n",
        "\n",
        "'''\n",
        "  Load Data Module for the AdaIN method by Huang and Belongie\n",
        "'''\n",
        "\n",
        "# dm = models.LITAdaINDataModule(train_batch_size = FLAGS['batch_size'] , \n",
        "#                 val_batch_size = FLAGS['test_batch_size'],\n",
        "#                 train_im_size = FLAGS['im_size'], \n",
        "#                 val_im_size = FLAGS['im_size']*FLAGS['val_size_mult'], \n",
        "#                 data_zip = [], \n",
        "#                 data_path = [FLAGS['datadir'],FLAGS['styledir']], \n",
        "#                 train_anno_path = FLAGS['labeldir'],\n",
        "#                 source_anno_path = FLAGS['styleanno']  \n",
        "#                 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n  Load Data Module for the AdaIN method by Huang and Belongie\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcer5p5PFWp3"
      },
      "source": [
        "##Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7m2_TnIFLWR"
      },
      "source": [
        "'''\n",
        "  need to load in test images and styles\n",
        "'''\n",
        "transform = transforms.Compose([ transforms.Resize(FLAGS['im_size']*2), transforms.RandomCrop((FLAGS['im_size'],FLAGS['im_size']))])\n",
        "val_transform = transforms.Compose([ transforms.Resize(FLAGS['im_size']*FLAGS['val_size_mult']), transforms.CenterCrop((FLAGS['im_size']*FLAGS['val_size_mult'],FLAGS['im_size']*FLAGS['val_size_mult']))])\n",
        "style_image = torch.Tensor([])\n",
        "val_image = torch.Tensor([])\n",
        "for s_im,v_im in zip(FLAGS['style_image_path'], FLAGS['val_image_path']): \n",
        "  s_image = transforms.ToTensor()(val_transform(Image.open(s_im).convert('RGB'))).unsqueeze(0)\n",
        "  v_image = transforms.ToTensor()(val_transform(Image.open(v_im).convert('RGB'))).unsqueeze(0)\n",
        "  style_image = torch.cat((style_image, s_image), dim = 0) \n",
        "  val_image = torch.cat((val_image,v_image), dim = 0) \n",
        "\n",
        "'''\n",
        "  comment next line if using AdaIN, otherwise, keep\n",
        "'''\n",
        "style_image = style_image[0].unsqueeze(0) \n",
        "val_image = val_image[0].unsqueeze(0) \n",
        "\n",
        "'''\n",
        "  load texture network\n",
        "'''\n",
        "# texture_net = models.LITTextureNet(style_image,\n",
        "#                              val_image,\n",
        "#                       batch_size = FLAGS['batch_size'],\n",
        "#                       normalization_method  = 'BatchNorm2d', \n",
        "#                       style_weight = 1e9, \n",
        "#                       content_weight = 2e3,\n",
        "#                       learning_rate = .001\n",
        "#                       )\n",
        "# texture_net_IN = models.LITTextureNet(style_image,\n",
        "#                              val_image,\n",
        "#                       batch_size = FLAGS['batch_size'],\n",
        "#                       normalization_method  = 'InstanceNorm2d', \n",
        "#                       style_weight = 1e9, \n",
        "#                       content_weight = 2e3,\n",
        "#                       learning_rate = .001)\n",
        "'''\n",
        "  load johnson et al's model\n",
        "'''\n",
        "# johnson_nst = models.LITFastNST(style_image,\n",
        "#                           val_image,\n",
        "#                       batch_size = FLAGS['batch_size'],\n",
        "#                       normalization_method =  'BatchNorm2d',\n",
        "#                       vgg_net = 'vgg16', \n",
        "#                       optimizer  = 'Adam', \n",
        "#                       learning_rate  = .001, \n",
        "#                       style_weight = 1. , \n",
        "#                       content_weight = 1., \n",
        "#                       pixel_weight = 10., \n",
        "#                       tv_weight = .00001)\n",
        "\n",
        "'''\n",
        "  load AdaIN module\n",
        "'''\n",
        "AdaIN = models.LITAdaIN(batch_size  = FLAGS['batch_size'],\n",
        "                      vgg_net  = 'vgg16', \n",
        "                      optimizer  = 'Adam', \n",
        "                      learning_rate  = 2e-4, \n",
        "                      style_weight = 100, \n",
        "                      content_weight = 1,\n",
        "                      test_style = style_image, \n",
        "                      test_image = val_image )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DBM7A58FY22"
      },
      "source": [
        "##Define Logger and Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gcTas9gFN45"
      },
      "source": [
        "'''\n",
        "  different tensor board loggers for different kind of modules\n",
        "'''\n",
        "#tb_logger_texture_net = pl_loggers.TensorBoardLogger(\"logs/texture_net\")\n",
        "#tb_logger_nst = pl_loggers.TensorBoardLogger('logs/Fast_NST')\n",
        "#logger_texture_net_IN = pl_loggers.TensorBoardLogger(\"logs/texture_net_IN\")\n",
        "ada_logger = pl_loggers.TensorBoardLogger('logs/adain')\n",
        "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
        "\n",
        "'''\n",
        "  Different trainers which load different metrics for different early stopping methods\n",
        "  and calling different trainers \n",
        "'''\n",
        "# texture_net_trainer = pl.Trainer(gpus = 1, logger = tb_logger_texture_net, log_every_n_steps= 10, stochastic_weight_avg = True, \n",
        "#                       callbacks = [EarlyStopping(monitor = 'style_loss',min_delta= .1, patience=25, verbose=True, mode=\"min\", check_finite=True, stopping_threshold=1.)])\n",
        "# texture_net_trainer_IN = pl.Trainer(gpus = 1, logger = logger_texture_net_IN, log_every_n_steps= 10, stochastic_weight_avg = True, \n",
        "#                       callbacks = [EarlyStopping(monitor = 'style_loss',min_delta= .1, patience=25, verbose=True, mode=\"min\", check_finite=True, stopping_threshold=1.)])\n",
        "# nst_trainer = pl.Trainer(gpus = 1, logger = tb_logger_nst, log_every_n_steps= 10, stochastic_weight_avg = True, \n",
        "#                       callbacks = [EarlyStopping(monitor = 'total loss',min_delta= .01, patience=25, verbose=True, mode=\"min\", check_finite=True, stopping_threshold=1.0)])\n",
        "\n",
        "ada_trainer = pl.Trainer(gpus = 1, logger = ada_logger, log_every_n_steps= 10, stochastic_weight_avg = True, \n",
        "                      callbacks = [EarlyStopping(monitor = 'total loss',min_delta= .001, patience=100, verbose=True, mode=\"min\", check_finite=True, stopping_threshold=.1), lr_monitor])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-gw_HDoFaqi"
      },
      "source": [
        "##Run  Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn0hK_1LFOZH"
      },
      "source": [
        "'''\n",
        "  running fit on different trainers to train different models\n",
        "'''\n",
        "# texture_net_trainer.fit(texture_net,dm)\n",
        "# texture_net_trainer_IN.fit(texture_net_IN,dm)\n",
        "# nst_trainer.fit(johnson_nst,dm) \n",
        "# ada_trainer.fit(AdaIN, dm) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7quj9KNmrYy"
      },
      "source": [
        "'''\n",
        "  Load TensorBoard Logger\n",
        "'''\n",
        "%reload_ext tensorboard\n",
        "%cd /content/\n",
        "%tensorboard --logdir logs/adain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eznh_kITzpOj"
      },
      "source": [
        "'''\n",
        "  saving different checkpoints for different models\n",
        "'''\n",
        "ada_trainer.save_checkpoint('/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/models/LitAdaIn.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKOGZS5RGFH5"
      },
      "source": [
        "'''\n",
        "  Printing sample output for trained AdaIN module\n",
        "'''\n",
        "AdaIN.eval() \n",
        "_,_,_,_, samples = AdaIN.texture_net(val_image, style_image)\n",
        "samples.clip_(1e-8, 1- 1e-8) \n",
        "samples = torch.cat((val_image, style_image, samples), dim =0)\n",
        "grid = torchvision.utils.make_grid(samples, nrow  = 4)\n",
        "print(grid.shape)\n",
        "fig = plt.figure(figsize = (10,10), dpi = 200) \n",
        "ax = fig.subplots() \n",
        "ax.imshow(grid.detach().permute(1,2,0))\n",
        "plt.axis('off')\n",
        "plt.show(block = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}