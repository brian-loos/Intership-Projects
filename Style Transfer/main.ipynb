{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main","private_outputs":true,"provenance":[],"collapsed_sections":["_OM_P3H4jxkW","M0cSudZwFBse","nob7wVEiFDbw"],"machine_shape":"hm","authorship_tag":"ABX9TyO/ONtJahdUayA9A+uLJ38D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mG3NbbphE7-v"},"source":["#Imports"]},{"cell_type":"code","metadata":{"id":"ugs_IL3LgvMr"},"source":["#!pip install --quiet \"pytorch-lightning>=1.3\" \"torchmetrics>=0.3\" \"torch>=1.6, <1.9\" \"torchvision\"\n","#!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n","!pip install pytorch-lightning\n","!pip install pickle5\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import pytorch_lightning as pl\n","import torchvision\n","import torchvision.models as visionmodels\n","import torchvision.transforms as transforms\n","import torch.utils.data.dataloader as DataLoader\n","import torch.utils.data.dataset as Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable,grad\n","from torchvision import datasets\n","from collections import *\n","import os\n","from torchvision.io import read_image\n","from datetime import datetime\n","import glob\n","from zipfile import ZipFile\n","from PIL import Image\n","from pytorch_lightning import loggers as pl_loggers\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive',force_remount= True)\n","pwd = '/content/'\n","base_dir = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer'\n","os.chdir(base_dir) \n","import models \n","os.chdir(pwd)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fU8rLUrsE-gK"},"source":["#Globals"]},{"cell_type":"code","metadata":{"id":"u0BR4FzhhPip"},"source":[" torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","\n","FLAGS = {} \n","FLAGS['datadir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/landscapes'\n","FLAGS['labeldir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/landscapes.csv'\n","FLAGS['batch_size'] = 8\n","FLAGS['test_batch_size'] = 2\n","FLAGS['learning_rate'] = .1\n","FLAGS['num_epochs'] = 2000\n","FLAGS['im_channels'] = 3\n","FLAGS['noise_scale_factor'] = .01\n","FLAGS['style_weight'] = 2e5\n","FLAGS['pixel_weight'] = 10. \n","FLAGS['content_weight'] = 1.\n","FLAGS['tv_weight'] = 1. \n","FLAGS['im_size'] = 256\n","FLAGS['val_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/big_sur.jpg' \n","FLAGS['style_image_path'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer/data/Paintings/swirl.png' \n","FLAGS['val_size_mult'] = 2\n","FLAGS['output_path'] = 'outputs/Texture_net_BN'\n","FLAGS['model_path'] = 'models/Texture_net_BN'\n","FLAGS['output_fname'] = 'test_output_'\n","FLAGS['model_fname'] = 'landscape_model'\n","FLAGS['home_dir'] = '/content/gdrive/MyDrive/Colab Notebooks/Style Transfer'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OM_P3H4jxkW"},"source":["#Shortcuts"]},{"cell_type":"code","metadata":{"id":"qWR-MIgaiTrh"},"source":["'''\n","  LITDataModule(train_batch_size = , \n","                val_batch_size = ,\n","                train_im_size = , \n","                val_im_size = , \n","                data_zip = [], \n","                data_path = [], \n","                train_anno_path = , \n","                test_anno_path = , \n","                val_anno_path = \n","                )\n","  LITAdaINDatamodule(train_batch_size = , \n","                val_batch_size = ,\n","                train_im_size = , \n","                val_im_size = , \n","                data_zip = [], \n","                data_path = [], \n","                train_anno_path = , \n","                test_anno_path = , \n","                val_anno_path = \n","                )\n","  LITTextureNet(style_image : torch.Tensor,\n","                      batch_size : int = 8,\n","                      normalization_method: str = 'BatchNorm2d', \n","                      num_layers : int = 6, \n","                      base_feat_maps: int = 8, \n","                      im_channels: int = 3,\n","                      vgg_net : str = 'vgg19', \n","                      optimizer : str = 'Adam', \n","                      learning_rate : float = .1, \n","                      betas : tuple = (.5,.99),\n","                      noise_scale_factor = .01,\n","                      style_weight = 1e5, \n","                      content_weight = 1.)\n","  LITFastNST(style_image : torch.Tensor,\n","                      batch_size : int = 8,\n","                      normalization_method: str = 'BatchNorm2d',\n","                      vgg_net : str = 'vgg19', \n","                      optimizer : str = 'Adam', \n","                      learning_rate : float = .1, \n","                      betas : tuple = (.5,.99),\n","                      noise_scale_factor = .01,\n","                      style_weight = 1., \n","                      content_weight = 1., \n","                      pixel_weight = 10., \n","                      tv_weight = 1.)\n","  LITAdaIN(batch_size : int = 8,\n","                      vgg_net : str = 'vgg19', \n","                      optimizer : str = 'Adam', \n","                      learning_rate : float = .1, \n","                      betas : tuple = (.5,.99),\n","                      noise_scale_factor = .01,\n","                      style_weight = 1., \n","                      content_weight = 1. )\n","    \n","  extract_features(data_dir,\n","                  output_fname,\n","                  vgg_net = \n","                  im_size =   \n","                  )\n","  \n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"394QkAMbjyxN"},"source":["#Test Area"]},{"cell_type":"markdown","metadata":{"id":"M0cSudZwFBse"},"source":["##Unzip Data"]},{"cell_type":"code","metadata":{"id":"D-X3dW-szd05"},"source":["# models.unzip_data(['data/cats','data/monet'],['/content/gdrive/MyDrive/Colab Notebooks/data/cats.zip', '/content/gdrive/MyDrive/Colab Notebooks/data/monet_jpg.zip'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nob7wVEiFDbw"},"source":["##Import data module"]},{"cell_type":"code","metadata":{"id":"rFlyGhfqk0wo"},"source":["dm = models.LITDataModule(train_batch_size = FLAGS['batch_size'] , \n","                val_batch_size = FLAGS['test_batch_size'],\n","                train_im_size = FLAGS['im_size'], \n","                val_im_size = FLAGS['im_size']*FLAGS['val_size_mult'], \n","                data_zip = [], \n","                data_path = [FLAGS['datadir'],FLAGS['datadir']], \n","                train_anno_path = FLAGS['labeldir'], \n","                test_anno_path = FLAGS['labeldir'], \n","                val_anno_path = FLAGS['labeldir']\n","                )\n","# dm = models.LITAdaINDataModule(train_batch_size = FLAGS['batch_size'] , \n","#                 val_batch_size = FLAGS['test_batch_size'],\n","#                 train_im_size = FLAGS['im_size'], \n","#                 val_im_size = FLAGS['im_size']*FLAGS['val_size_mult'], \n","#                 data_zip = [], \n","#                 data_path = ['data/cats','data/monet'], \n","#                 train_anno_path = '/content/gdrive/MyDrive/Colab Notebooks/data/monet.pkl', \n","#                 test_anno_path = '/content/gdrive/MyDrive/Colab Notebooks/data/monet.pkl', \n","#                 val_anno_path ='/content/gdrive/MyDrive/Colab Notebooks/data/monet.pkl'\n","#                 )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pcer5p5PFWp3"},"source":["##Define Model"]},{"cell_type":"code","metadata":{"id":"f7m2_TnIFLWR"},"source":["transform = transforms.Compose([ transforms.Resize(FLAGS['im_size']), transforms.CenterCrop((FLAGS['im_size'],FLAGS['im_size']))])\n","val_transform = transforms.Compose([ transforms.Resize(FLAGS['im_size']*FLAGS['val_size_mult']), transforms.CenterCrop((FLAGS['im_size']*FLAGS['val_size_mult'],FLAGS['im_size']*FLAGS['val_size_mult']))])\n","style_image = transforms.ToTensor()(transform(Image.open(FLAGS['style_image_path']).convert('RGB'))).unsqueeze(0)\n","val_image = transforms.ToTensor()(transform(Image.open(FLAGS['val_image_path']).convert('RGB'))).unsqueeze(0)\n","texture_net = models.LITTextureNet(style_image,\n","                             val_image,\n","                      batch_size = FLAGS['batch_size'],\n","                      normalization_method  = 'BatchNorm2d', \n","                      style_weight = 2e4, \n","                      content_weight = 1e6)\n","johnson_nst = models.LITFastNST(style_image,\n","                          val_image,\n","                      batch_size = FLAGS['batch_size'],\n","                      normalization_method =  'BatchNorm2d',\n","                      vgg_net = 'vgg19', \n","                      optimizer  = 'Adam', \n","                      learning_rate  = .001, \n","                      style_weight = .01, \n","                      content_weight = 100., \n","                      pixel_weight = 1000., \n","                      tv_weight = .0001)\n","# model = models.LITAdaIN(batch_size  = FLAGS['batch_size'],\n","#                       vgg_net  = 'vgg16', \n","#                       optimizer  = 'Adam', \n","#                       learning_rate  = FLAGS['learning_rate'], \n","#                       style_weight = 1., \n","#                       content_weight = 1. )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6DBM7A58FY22"},"source":["##Define Logger and Trainer"]},{"cell_type":"code","metadata":{"id":"6gcTas9gFN45"},"source":["tb_logger_texture_net = pl_loggers.TensorBoardLogger(\"logs/texture_net\")\n","tb_logger_nst = pl_loggers.TensorBoardLogger('logs/Fast_NST')\n","texture_net_trainer = pl.Trainer(gpus = 1, logger = tb_logger_texture_net, log_every_n_steps= 10, stochastic_weight_avg = True, \n","                      callbacks = [EarlyStopping(monitor = 'style_loss',min_delta= 1e2, patience=25, verbose=True, mode=\"min\", check_finite=True, stopping_threshold=1e5)])\n","nst_trainer = pl.Trainer(gpus = 1, logger = tb_logger_nst, log_every_n_steps= 10, stochastic_weight_avg = True, \n","                      callbacks = [EarlyStopping(monitor = 'style loss',min_delta= .1, patience=25, verbose=True, mode=\"min\", check_finite=True, stopping_threshold=1.0)])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-gw_HDoFaqi"},"source":["##Run  Model"]},{"cell_type":"code","metadata":{"id":"hn0hK_1LFOZH"},"source":["#texture_net_trainer.fit(texture_net,dm)\n","nst_trainer.fit(johnson_nst,dm) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7quj9KNmrYy"},"source":["%load_ext tensorboard\n","%cd /content/\n","%tensorboard --logdir logs/texture_net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"prkjxnuM_cbE"},"source":["%tensorboard --logdir logs/Fast_NST"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKOGZS5RGFH5"},"source":[""],"execution_count":null,"outputs":[]}]}